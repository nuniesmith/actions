name: 🚀 Unified Service Management

on:
  workflow_call:
    inputs:
      service_name:
        description: 'Name of the service to manage (e.g., fks, nginx, ats)'
        required: true
        type: string
      
      action_type:
        description: 'Action to perform'
        required: false
        type: string
        default: 'deploy'
        # Options: deploy, destroy, health-check, restart
      
      deployment_mode:
        description: 'Deployment mode'
        required: false
        type: string
        default: 'full-deploy'
        # Options: full-deploy, update-only, restart-only, code-only
      
      # 🎯 New Options You Requested
      skip_tests:
        description: 'Skip running code tests'
        required: false
        type: boolean
        default: false
      
      skip_docker_build:
        description: 'Skip building Docker images'
        required: false
        type: boolean
        default: false
      
      build_docker_on_changes:
        description: 'Only build Docker if code/Dockerfile changed'
        required: false
        type: boolean
        default: true
      
      overwrite_server:
        description: 'Destroy and recreate Linode server'
        required: false
        type: boolean
        default: false
      
      # Server Configuration
      server_type:
        description: 'Linode server type'
        required: false
        type: string
        default: 'g6-standard-2'
      
      target_region:
        description: 'Linode region'
        required: false
        type: string
        default: 'ca-central'
      
      domain_suffix:
        description: 'Domain suffix (e.g., 7gram.xyz)'
        required: false
        type: string
        default: '7gram.xyz'
      
      # Feature Toggles
      enable_monitoring:
        description: 'Enable Netdata monitoring'
        required: false
        type: boolean
        default: true
      
      enable_backups:
        description: 'Enable Linode backups'
        required: false
        type: boolean
        default: false
      
      # Destroy Options (for destroy action)
      destroy_scope:
        description: 'What to destroy (for destroy action)'
        required: false
        type: string
        default: 'service-only'
      
      confirm_destruction:
        description: 'Type "DESTROY" to confirm destruction'
        required: false
        type: string

    secrets:
      # Core Infrastructure
      LINODE_TOKEN:
        required: true
      SERVICE_ROOT_PASSWORD:
        required: true
      
      # User Management
      JORDAN_PASSWORD:
        required: true
      ACTIONS_USER_PASSWORD:
        required: true
      
      # VPN & Networking
      TAILSCALE_AUTH_KEY:
        required: true
      TAILSCALE_OAUTH_CLIENT_ID:
        required: false
      TAILSCALE_OAUTH_SECRET:
        required: false
      
      # Monitoring (Optional)
      NETDATA_CLAIM_TOKEN:
        required: false
      NETDATA_CLAIM_ROOM:
        required: false
      
      # DNS Management (Optional)
      CLOUDFLARE_API_TOKEN:
        required: false
      CLOUDFLARE_ZONE_ID:
        required: false
      
      # Container Registry (Optional)
      DOCKER_USERNAME:
        required: false
      DOCKER_TOKEN:
        required: false
      
      # Notifications (Optional)
      DISCORD_WEBHOOK:
        required: false

  workflow_dispatch:
    inputs:
      service_name:
        description: 'Name of the service to manage'
        required: true
        type: choice
        options:
          - 'fks'
          - 'nginx'
          - 'ats'
          - 'custom'
      
      action_type:
        description: 'Action to perform'
        required: true
        type: choice
        options:
          - 'deploy'
          - 'destroy'
          - 'health-check'
          - 'restart'
        default: 'deploy'
      
      deployment_mode:
        description: 'Deployment mode (for deploy action)'
        required: false
        type: choice
        options:
          - 'full-deploy'
          - 'update-only'
          - 'restart-only'
          - 'code-only'
        default: 'full-deploy'
      
      # 🎯 Your Requested Options
      skip_tests:
        description: 'Skip running code tests'
        required: false
        type: boolean
        default: false
      
      skip_docker_build:
        description: 'Skip building Docker images'
        required: false
        type: boolean
        default: false
      
      build_docker_on_changes:
        description: 'Only build Docker if code/Dockerfile changed'
        required: false
        type: boolean
        default: true
      
      overwrite_server:
        description: 'Destroy and recreate Linode server'
        required: false
        type: boolean
        default: false
      
      # Destroy Options (for destroy action)
      destroy_scope:
        description: 'What to destroy (for destroy action)'
        required: false
        type: choice
        options:
          - 'service-only'
          - 'full-server'
          - 'reset-service'
        default: 'service-only'
      
      confirm_destruction:
        description: 'Type "DESTROY" to confirm destruction'
        required: false
        type: string
      
      # Server Configuration
      server_type:
        description: 'Linode server type'
        required: false
        type: choice
        options:
          - 'g6-nanode-1'          # 1GB RAM
          - 'g6-standard-1'        # 2GB RAM
          - 'g6-standard-2'        # 4GB RAM
          - 'g6-standard-4'        # 8GB RAM
          - 'g6-standard-8'        # 16GB RAM
        default: 'g6-standard-2'

env:
  SERVICE_NAME: ${{ inputs.service_name }}
  ACTION_TYPE: ${{ inputs.action_type }}
  DEPLOYMENT_MODE: ${{ inputs.deployment_mode }}
  SERVER_TYPE: ${{ inputs.server_type }}
  TARGET_REGION: ${{ inputs.target_region || 'ca-central' }}
  DOMAIN_SUFFIX: ${{ inputs.domain_suffix || '7gram.xyz' }}
  FULL_DOMAIN: ${{ inputs.service_name }}.${{ inputs.domain_suffix || '7gram.xyz' }}
  
  # Your requested options
  SKIP_TESTS: ${{ inputs.skip_tests }}
  SKIP_DOCKER_BUILD: ${{ inputs.skip_docker_build }}
  BUILD_DOCKER_ON_CHANGES: ${{ inputs.build_docker_on_changes }}
  OVERWRITE_SERVER: ${{ inputs.overwrite_server }}

jobs:
  # ============================================================================
  # Pre-flight Checks & Validation
  # ============================================================================
  preflight-checks:
    name: 🛫 Pre-flight Checks
    runs-on: ubuntu-latest
    outputs:
      action_validated: ${{ steps.validate-action.outputs.validated }}
      should_destroy: ${{ steps.validate-action.outputs.should_destroy }}
      should_deploy: ${{ steps.validate-action.outputs.should_deploy }}
      should_health_check: ${{ steps.validate-action.outputs.should_health_check }}
      should_overwrite_server: ${{ steps.validate-action.outputs.should_overwrite_server }}
      destroy_confirmed: ${{ steps.validate-destroy.outputs.confirmed }}
      code_changed: ${{ steps.check-changes.outputs.code_changed }}
      docker_build_needed: ${{ steps.check-changes.outputs.docker_build_needed }}
    
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch full history for change detection

      - name: 🎯 Validate Action Type
        id: validate-action
        run: |
          echo "🎯 Validating action: ${{ env.ACTION_TYPE }}"
          
          case "${{ env.ACTION_TYPE }}" in
            "deploy")
              echo "should_deploy=true" >> $GITHUB_OUTPUT
              echo "should_destroy=false" >> $GITHUB_OUTPUT
              echo "should_health_check=false" >> $GITHUB_OUTPUT
              ;;
            "destroy")
              echo "should_deploy=false" >> $GITHUB_OUTPUT
              echo "should_destroy=true" >> $GITHUB_OUTPUT
              echo "should_health_check=false" >> $GITHUB_OUTPUT
              ;;
            "health-check")
              echo "should_deploy=false" >> $GITHUB_OUTPUT
              echo "should_destroy=false" >> $GITHUB_OUTPUT
              echo "should_health_check=true" >> $GITHUB_OUTPUT
              ;;
            "restart")
              echo "should_deploy=true" >> $GITHUB_OUTPUT
              echo "should_destroy=false" >> $GITHUB_OUTPUT
              echo "should_health_check=false" >> $GITHUB_OUTPUT
              ;;
            *)
              echo "❌ Invalid action type: ${{ env.ACTION_TYPE }}"
              exit 1
              ;;
          esac
          
          # Check if server should be overwritten
          if [[ "${{ env.OVERWRITE_SERVER }}" == "true" && "${{ env.ACTION_TYPE }}" == "deploy" ]]; then
            echo "should_overwrite_server=true" >> $GITHUB_OUTPUT
            echo "⚠️ Server will be overwritten (destroyed and recreated)"
          else
            echo "should_overwrite_server=false" >> $GITHUB_OUTPUT
          fi
          
          echo "validated=true" >> $GITHUB_OUTPUT

      - name: ⚠️ Validate Destruction Request
        id: validate-destroy
        if: steps.validate-action.outputs.should_destroy == 'true' || steps.validate-action.outputs.should_overwrite_server == 'true'
        run: |
          # For server overwrite during deployment, skip confirmation requirement
          if [[ "${{ steps.validate-action.outputs.should_overwrite_server }}" == "true" && "${{ env.ACTION_TYPE }}" == "deploy" ]]; then
            echo "✅ Server overwrite confirmed (deploy mode)"
            echo "confirmed=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # For explicit destroy actions, require confirmation
          if [[ "${{ inputs.confirm_destruction }}" != "DESTROY" ]]; then
            echo "❌ Destruction not confirmed. You must type 'DESTROY' exactly."
            echo "confirmed=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          echo "✅ Destruction confirmed for ${{ env.SERVICE_NAME }}"
          echo "confirmed=true" >> $GITHUB_OUTPUT

      - name: 🔍 Check for Code Changes
        id: check-changes
        if: steps.validate-action.outputs.should_deploy == 'true'
        run: |
          echo "🔍 Checking for code and Docker changes..."
          
          # Check if this is the first commit or if we should build anyway
          if [[ $(git rev-list --count HEAD) -le 1 ]] || [[ "${{ env.BUILD_DOCKER_ON_CHANGES }}" == "false" ]]; then
            echo "First commit or change detection disabled - assuming changes exist"
            echo "code_changed=true" >> $GITHUB_OUTPUT
            echo "docker_build_needed=true" >> $GITHUB_OUTPUT
          else
            # Check for changes in the last commit
            CHANGED_FILES=$(git diff --name-only HEAD~1 HEAD)
            echo "Changed files: $CHANGED_FILES"
            
            # Check if code files changed (exclude docs, configs, etc.)
            CODE_CHANGED="false"
            if echo "$CHANGED_FILES" | grep -E '\.(js|ts|py|go|java|cpp|c|rs|php)$' > /dev/null; then
              CODE_CHANGED="true"
              echo "✅ Code files changed"
            fi
            
            # Check if Docker-related files changed
            DOCKER_BUILD_NEEDED="false"
            if echo "$CHANGED_FILES" | grep -E '(Dockerfile|docker-compose|requirements|package\.json|go\.mod|Cargo\.toml)' > /dev/null; then
              DOCKER_BUILD_NEEDED="true"
              echo "✅ Docker-related files changed"
            fi
            
            # If build_docker_on_changes is true, only build if changes detected
            if [[ "${{ env.BUILD_DOCKER_ON_CHANGES }}" == "true" ]]; then
              if [[ "$CODE_CHANGED" == "true" || "$DOCKER_BUILD_NEEDED" == "true" ]]; then
                DOCKER_BUILD_NEEDED="true"
              else
                DOCKER_BUILD_NEEDED="false"
                echo "ℹ️ No relevant changes detected - skipping Docker build"
              fi
            fi
            
            echo "code_changed=$CODE_CHANGED" >> $GITHUB_OUTPUT
            echo "docker_build_needed=$DOCKER_BUILD_NEEDED" >> $GITHUB_OUTPUT
          fi

      - name: 🔐 Validate Secrets
        env:
          LINODE_TOKEN: ${{ secrets.LINODE_TOKEN }}
          SERVICE_ROOT_PASSWORD: ${{ secrets.SERVICE_ROOT_PASSWORD }}
          JORDAN_PASSWORD: ${{ secrets.JORDAN_PASSWORD }}
          ACTIONS_USER_PASSWORD: ${{ secrets.ACTIONS_USER_PASSWORD }}
          TAILSCALE_AUTH_KEY: ${{ secrets.TAILSCALE_AUTH_KEY }}
        run: |
          echo "🔐 Validating required secrets..."
          
          MISSING_SECRETS=()
          [[ -z "$LINODE_TOKEN" ]] && MISSING_SECRETS+=("LINODE_TOKEN")
          [[ -z "$SERVICE_ROOT_PASSWORD" ]] && MISSING_SECRETS+=("SERVICE_ROOT_PASSWORD")
          [[ -z "$JORDAN_PASSWORD" ]] && MISSING_SECRETS+=("JORDAN_PASSWORD")
          [[ -z "$ACTIONS_USER_PASSWORD" ]] && MISSING_SECRETS+=("ACTIONS_USER_PASSWORD")
          [[ -z "$TAILSCALE_AUTH_KEY" ]] && MISSING_SECRETS+=("TAILSCALE_AUTH_KEY")
          
          if [[ ${#MISSING_SECRETS[@]} -gt 0 ]]; then
            echo "❌ Missing required secrets:"
            printf '  - %s\n' "${MISSING_SECRETS[@]}"
            exit 1
          fi
          
          echo "✅ All required secrets validated"

  # ============================================================================
  # Code Testing (Optional)
  # ============================================================================
  run-tests:
    name: 🧪 Run Tests
    runs-on: ubuntu-latest
    needs: preflight-checks
    if: needs.preflight-checks.outputs.should_deploy == 'true' && inputs.skip_tests == false
    
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4

      - name: 🧪 Auto-detect and Run Tests
        run: |
          echo "🧪 Auto-detecting test framework..."
          
          # Node.js/JavaScript tests
          if [[ -f "package.json" ]]; then
            echo "📦 Node.js project detected"
            if command -v npm &> /dev/null; then
              echo "Installing dependencies..."
              npm install
              
              if npm run test --if-present; then
                echo "✅ Node.js tests passed"
              else
                echo "⚠️ Node.js tests failed or no test script found"
              fi
            fi
          fi
          
          # Python tests
          if [[ -f "requirements.txt" ]] || [[ -f "pyproject.toml" ]] || [[ -f "setup.py" ]]; then
            echo "🐍 Python project detected"
            if command -v python3 &> /dev/null; then
              if [[ -f "requirements.txt" ]]; then
                pip install -r requirements.txt
              fi
              
              # Try different test runners
              if python -m pytest --version &> /dev/null && find . -name "*test*.py" | grep -q .; then
                echo "Running pytest..."
                python -m pytest
              elif python -m unittest discover -s . -p "*test*.py" 2>/dev/null; then
                echo "✅ Python unittest tests passed"
              else
                echo "ℹ️ No Python tests found or test framework not available"
              fi
            fi
          fi
          
          # Go tests
          if [[ -f "go.mod" ]]; then
            echo "🔷 Go project detected"
            if command -v go &> /dev/null; then
              go test ./...
              echo "✅ Go tests passed"
            fi
          fi
          
          echo "✅ Test phase complete"

  # ============================================================================
  # Docker Build (Conditional)
  # ============================================================================
  build-docker:
    name: 🐳 Build Docker Images
    runs-on: ubuntu-latest
    needs: [preflight-checks, run-tests]
    if: |
      always() && 
      needs.preflight-checks.outputs.should_deploy == 'true' && 
      inputs.skip_docker_build == false && 
      needs.preflight-checks.outputs.docker_build_needed == 'true' &&
      (needs.run-tests.result == 'success' || needs.run-tests.result == 'skipped')
    
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4

      - name: 🔑 Login to Docker Hub
        env:
          DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
          DOCKER_TOKEN: ${{ secrets.DOCKER_TOKEN }}
        if: env.DOCKER_USERNAME != '' && env.DOCKER_TOKEN != ''
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_TOKEN }}

      - name: 🐳 Build and Push Docker Images
        run: |
          echo "🐳 Building Docker images for ${{ env.SERVICE_NAME }}..."
          
          # Auto-detect Docker files
          if [[ -f "Dockerfile" ]]; then
            echo "📋 Found Dockerfile - building main image"
            
            IMAGE_TAG="${{ secrets.DOCKER_USERNAME }}/${{ env.SERVICE_NAME }}:latest"
            
            # Build the image
            docker build -t "$IMAGE_TAG" .
            
            # Push if credentials available
            if [[ -n "${{ secrets.DOCKER_USERNAME }}" ]]; then
              echo "📤 Pushing image: $IMAGE_TAG"
              docker push "$IMAGE_TAG"
            else
              echo "ℹ️ No Docker credentials - image built locally only"
            fi
          fi
          
          # Build using docker-compose if available
          if [[ -f "docker-compose.yml" ]]; then
            echo "📋 Found docker-compose.yml - building services"
            docker-compose build
            
            if [[ -n "${{ secrets.DOCKER_USERNAME }}" ]]; then
              echo "📤 Pushing compose images..."
              docker-compose push || echo "⚠️ Some images may not have push configured"
            fi
          fi
          
          echo "✅ Docker build complete"

  # ============================================================================
  # Server Destruction (if overwrite requested)
  # ============================================================================
  destroy-existing-server:
    name: 💥 Destroy Existing Server
    runs-on: ubuntu-latest
    needs: [preflight-checks, build-docker]
    if: |
      always() && 
      needs.preflight-checks.outputs.should_overwrite_server == 'true' &&
      needs.preflight-checks.outputs.destroy_confirmed == 'true' &&
      (needs.build-docker.result == 'success' || needs.build-docker.result == 'skipped')
    
    steps:
      - name: � Checkout repository
        uses: actions/checkout@v4

      - name: �🔧 Setup Linode CLI
        env:
          LINODE_CLI_TOKEN: ${{ secrets.LINODE_TOKEN }}
        run: |
          pip install linode-cli
          # Configure via environment variable to avoid config file issues
          export LINODE_CLI_TOKEN="${{ secrets.LINODE_TOKEN }}"
          # Test CLI access
          linode-cli --version

      - name: 🔗 Setup Cleanup Connection (Tailscale)
        env:
          TAILSCALE_OAUTH_CLIENT_ID: ${{ secrets.TAILSCALE_OAUTH_CLIENT_ID }}
          TAILSCALE_OAUTH_SECRET: ${{ secrets.TAILSCALE_OAUTH_SECRET }}
        if: env.TAILSCALE_OAUTH_CLIENT_ID != '' && env.TAILSCALE_OAUTH_SECRET != ''
        uses: tailscale/github-action@v2
        with:
          oauth-client-id: ${{ secrets.TAILSCALE_OAUTH_CLIENT_ID }}
          oauth-secret: ${{ secrets.TAILSCALE_OAUTH_SECRET }}
        continue-on-error: true

      - name: 🧹 Cleanup Services Before Destruction
        env:
          LINODE_CLI_TOKEN: ${{ secrets.LINODE_TOKEN }}
        run: |
          echo "🧹 Cleaning up external service registrations..."
          
          # Find existing server
          EXISTING_SERVER=$(linode-cli linodes list --text --no-headers | grep "${{ env.SERVICE_NAME }}" | head -1)
          
          if [[ -n "$EXISTING_SERVER" ]]; then
            SERVER_ID=$(echo "$EXISTING_SERVER" | cut -f1)
            SERVER_LABEL=$(echo "$EXISTING_SERVER" | cut -f2)
            
            # Try to get server IP for cleanup
            SERVER_INFO=$(linode-cli linodes view "$SERVER_ID" --text --no-headers)
            SERVER_IP_COL4=$(echo "$SERVER_INFO" | cut -f4)
            SERVER_IP_COL5=$(echo "$SERVER_INFO" | cut -f5)
            SERVER_IP_COL6=$(echo "$SERVER_INFO" | cut -f6)
            SERVER_IP_COL7=$(echo "$SERVER_INFO" | cut -f7)
            
            SERVER_IP=""
            for IP in "$SERVER_IP_COL4" "$SERVER_IP_COL5" "$SERVER_IP_COL6" "$SERVER_IP_COL7"; do
              if [[ "$IP" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
                SERVER_IP="$IP"
                break
              fi
            done
            
            if [[ -n "$SERVER_IP" ]]; then
              echo "🔍 Found server to cleanup: $SERVER_LABEL ($SERVER_IP)"
              
              # Generate SSH key for cleanup access
              ssh-keygen -t rsa -b 4096 -f ~/.ssh/cleanup_key -N "" -C "cleanup-${{ env.SERVICE_NAME }}" || true
              
              # Try to connect and cleanup (with error handling)
              if timeout 30 ssh -i ~/.ssh/cleanup_key -o StrictHostKeyChecking=no -o ConnectTimeout=10 root@$SERVER_IP "echo 'Cleanup connection successful'" 2>/dev/null; then
                echo "🔗 Connected to server for cleanup..."
                
                # Cleanup Tailscale
                echo "🧹 Removing Tailscale device..."
                ssh -i ~/.ssh/cleanup_key -o StrictHostKeyChecking=no root@$SERVER_IP "tailscale logout || true" || true
                
                # Cleanup Netdata cloud connection
                echo "🧹 Cleaning up Netdata cloud connection..."
                ssh -i ~/.ssh/cleanup_key -o StrictHostKeyChecking=no root@$SERVER_IP "systemctl stop netdata || true; rm -rf /var/lib/netdata/cloud.d/ || true; rm -rf /var/lib/netdata/registry/ || true" || true
                
                echo "✅ Service cleanup completed"
              else
                echo "⚠️ Could not connect to server for cleanup (server may be down or SSH not configured)"
              fi
            else
              echo "⚠️ Could not determine server IP for cleanup"
            fi
            
            # Additional Tailscale cleanup via API if available
            if command -v tailscale >/dev/null 2>&1; then
              echo "🔗 Attempting Tailscale device cleanup via local client..."
              # List devices and try to remove the one matching our service name
              tailscale status --json 2>/dev/null | jq -r '.Peer[] | select(.HostName == "${{ env.SERVICE_NAME }}") | .ID' | while read device_id; do
                if [[ -n "$device_id" ]]; then
                  echo "Removing Tailscale device: $device_id"
                  tailscale delete "$device_id" || true
                fi
              done || true
            fi
          else
            echo "ℹ️ No existing server found for cleanup"
          fi

      - name: 💥 Destroy Existing Server
        env:
          LINODE_CLI_TOKEN: ${{ secrets.LINODE_TOKEN }}
        run: |
          echo "💥 Looking for existing ${{ env.SERVICE_NAME }} server to destroy..."
          
          # Find existing server
          EXISTING_SERVER=$(linode-cli linodes list --text --no-headers | grep "${{ env.SERVICE_NAME }}" | head -1)
          
          if [[ -n "$EXISTING_SERVER" ]]; then
            SERVER_ID=$(echo "$EXISTING_SERVER" | cut -f1)
            SERVER_LABEL=$(echo "$EXISTING_SERVER" | cut -f2)
            
            echo "🔥 Destroying server: $SERVER_LABEL (ID: $SERVER_ID)"
            linode-cli linodes delete "$SERVER_ID"
            
            echo "⏳ Waiting for server destruction to complete..."
            sleep 30
            
            echo "✅ Server destroyed successfully"
          else
            echo "ℹ️ No existing server found for ${{ env.SERVICE_NAME }}"
          fi

  # ============================================================================
  # Main Destroy Job (for destroy action)
  # ============================================================================
  destroy-service:
    name: 🗑️ Destroy Service
    runs-on: ubuntu-latest
    needs: preflight-checks
    if: needs.preflight-checks.outputs.should_destroy == 'true' && needs.preflight-checks.outputs.destroy_confirmed == 'true'
    
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4

      - name: 🔧 Setup Linode CLI
        if: inputs.destroy_scope == 'full-server'
        env:
          LINODE_CLI_TOKEN: ${{ secrets.LINODE_TOKEN }}
        run: |
          pip install linode-cli
          # Configure via environment variable to avoid config file issues
          export LINODE_CLI_TOKEN="${{ secrets.LINODE_TOKEN }}"
          # Test CLI access
          linode-cli --version

      - name: � Setup Cleanup Connection (Tailscale)
        env:
          TAILSCALE_OAUTH_CLIENT_ID: ${{ secrets.TAILSCALE_OAUTH_CLIENT_ID }}
          TAILSCALE_OAUTH_SECRET: ${{ secrets.TAILSCALE_OAUTH_SECRET }}
        if: env.TAILSCALE_OAUTH_CLIENT_ID != '' && env.TAILSCALE_OAUTH_SECRET != '' && inputs.destroy_scope == 'full-server'
        uses: tailscale/github-action@v2
        with:
          oauth-client-id: ${{ secrets.TAILSCALE_OAUTH_CLIENT_ID }}
          oauth-secret: ${{ secrets.TAILSCALE_OAUTH_SECRET }}
        continue-on-error: true

      - name: �🗑️ Execute Destruction
        env:
          LINODE_CLI_TOKEN: ${{ secrets.LINODE_TOKEN }}
        run: |
          echo "🗑️ Destroying ${{ env.SERVICE_NAME }}..."
          echo "Scope: ${{ inputs.destroy_scope }}"
          
          case "${{ inputs.destroy_scope }}" in
            "service-only")
              echo "🛑 Stopping service containers only..."
              # Logic for service-only destruction
              ;;
            "reset-service")
              echo "🧹 Resetting service to clean state..."
              # Logic for service reset
              ;;
            "full-server")
              echo "💥 Destroying entire server with cleanup..."
              
              # Find and cleanup server before destruction
              SERVER_INFO=$(linode-cli linodes list --text --no-headers | grep "${{ env.SERVICE_NAME }}" | head -1)
              if [[ -n "$SERVER_INFO" ]]; then
                SERVER_ID=$(echo "$SERVER_INFO" | cut -f1)
                SERVER_LABEL=$(echo "$SERVER_INFO" | cut -f2)
                
                # Try to get server IP for cleanup
                SERVER_DETAILS=$(linode-cli linodes view "$SERVER_ID" --text --no-headers)
                SERVER_IP_COL4=$(echo "$SERVER_DETAILS" | cut -f4)
                SERVER_IP_COL5=$(echo "$SERVER_DETAILS" | cut -f5)
                SERVER_IP_COL6=$(echo "$SERVER_DETAILS" | cut -f6)
                SERVER_IP_COL7=$(echo "$SERVER_DETAILS" | cut -f7)
                
                SERVER_IP=""
                for IP in "$SERVER_IP_COL4" "$SERVER_IP_COL5" "$SERVER_IP_COL6" "$SERVER_IP_COL7"; do
                  if [[ "$IP" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
                    SERVER_IP="$IP"
                    break
                  fi
                done
                
                if [[ -n "$SERVER_IP" ]]; then
                  echo "🧹 Cleaning up services on server before destruction..."
                  
                  # Generate SSH key for cleanup
                  ssh-keygen -t rsa -b 4096 -f ~/.ssh/cleanup_key -N "" -C "cleanup-${{ env.SERVICE_NAME }}" || true
                  
                  # Try to connect and cleanup
                  if timeout 30 ssh -i ~/.ssh/cleanup_key -o StrictHostKeyChecking=no -o ConnectTimeout=10 root@$SERVER_IP "echo 'Cleanup connection successful'" 2>/dev/null; then
                    echo "🔗 Connected for cleanup..."
                    
                    # Cleanup Tailscale
                    ssh -i ~/.ssh/cleanup_key -o StrictHostKeyChecking=no root@$SERVER_IP "tailscale logout || true" || true
                    
                    # Cleanup Netdata
                    ssh -i ~/.ssh/cleanup_key -o StrictHostKeyChecking=no root@$SERVER_IP "systemctl stop netdata || true; rm -rf /var/lib/netdata/cloud.d/ || true" || true
                    
                    echo "✅ Service cleanup completed"
                  else
                    echo "⚠️ Could not connect for cleanup"
                  fi
                fi
                
                echo "Destroying: $SERVER_LABEL (ID: $SERVER_ID)"
                linode-cli linodes delete "$SERVER_ID"
                echo "✅ Server destroyed"
              else
                echo "⚠️ No server found for ${{ env.SERVICE_NAME }}"
              fi
              ;;
          esac

  # ============================================================================
  # Server Infrastructure Setup
  # ============================================================================
  setup-infrastructure:
    name: 🏗️ Setup Infrastructure
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [preflight-checks, destroy-existing-server, build-docker]
    if: |
      always() && 
      needs.preflight-checks.outputs.should_deploy == 'true' &&
      (needs.destroy-existing-server.result == 'success' || needs.destroy-existing-server.result == 'skipped') &&
      (needs.build-docker.result == 'success' || needs.build-docker.result == 'skipped')
    outputs:
      server_ip: ${{ steps.create-server.outputs.server_ip }}
      server_id: ${{ steps.create-server.outputs.server_id }}
      tailscale_ip: ${{ steps.stage2-setup.outputs.tailscale_ip }}
    
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4

      - name: 🔧 Setup Linode CLI
        env:
          LINODE_CLI_TOKEN: ${{ secrets.LINODE_TOKEN }}
        run: |
          pip install linode-cli
          # Configure via environment variable to avoid config file issues
          export LINODE_CLI_TOKEN="${{ secrets.LINODE_TOKEN }}"
          # Test CLI access
          linode-cli --version

      - name: 🚀 Create or Find Server
        id: create-server
        env:
          LINODE_CLI_TOKEN: ${{ secrets.LINODE_TOKEN }}
        run: |
          echo "🚀 Managing Linode server for ${{ env.SERVICE_NAME }}..."
          
          # Check if server already exists (unless we just destroyed it)
          if [[ "${{ inputs.overwrite_server }}" != "true" ]]; then
            EXISTING_SERVER=$(linode-cli linodes list --text --no-headers | grep "${{ env.SERVICE_NAME }}" | head -1)
            if [[ -n "$EXISTING_SERVER" ]]; then
              echo "🔍 Debug - Found existing server:"
              echo "$EXISTING_SERVER"
              
              SERVER_ID=$(echo "$EXISTING_SERVER" | cut -f1)
              # Try different columns for IP address
              SERVER_IP_COL4=$(echo "$EXISTING_SERVER" | cut -f4)
              SERVER_IP_COL5=$(echo "$EXISTING_SERVER" | cut -f5)
              SERVER_IP_COL6=$(echo "$EXISTING_SERVER" | cut -f6)
              SERVER_IP_COL7=$(echo "$EXISTING_SERVER" | cut -f7)
              
              echo "IP candidates: Col4='$SERVER_IP_COL4', Col5='$SERVER_IP_COL5', Col6='$SERVER_IP_COL6', Col7='$SERVER_IP_COL7'"
              
              # Use the first valid IP address we find
              for IP in "$SERVER_IP_COL4" "$SERVER_IP_COL5" "$SERVER_IP_COL6" "$SERVER_IP_COL7"; do
                if [[ "$IP" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
                  SERVER_IP="$IP"
                  break
                fi
              done
              
              if [[ -z "$SERVER_IP" ]]; then
                echo "❌ Could not extract IP address from server info"
                exit 1
              fi
              
              echo "✅ Using existing server: $SERVER_IP (ID: $SERVER_ID)"
              echo "server_ip=$SERVER_IP" >> $GITHUB_OUTPUT
              echo "server_id=$SERVER_ID" >> $GITHUB_OUTPUT
              exit 0
            fi
          fi
          
          # Create new server
          SERVER_LABEL="${{ env.SERVICE_NAME }}"
          echo "🆕 Creating new server: $SERVER_LABEL"
          
          # Generate SSH key for this deployment
          echo "🔑 Generating SSH key for server access..."
          ssh-keygen -t rsa -b 4096 -f ~/.ssh/linode_deployment_key -N "" -C "github-actions-${{ env.SERVICE_NAME }}"
          
          # Get the public key content for server authorization
          SSH_PUBLIC_KEY=$(cat ~/.ssh/linode_deployment_key.pub)
          echo "🔑 SSH Public Key: $SSH_PUBLIC_KEY"
          
          # Store the private key (base64 encoded for safe storage)
          SSH_PRIVATE_KEY=$(base64 -w 0 ~/.ssh/linode_deployment_key)
          echo "ssh_private_key=$SSH_PRIVATE_KEY" >> $GITHUB_OUTPUT
          
          echo "🚀 Creating server with SSH key authentication..."
          RESULT=$(linode-cli linodes create \
            --type "${{ env.SERVER_TYPE }}" \
            --region "${{ env.TARGET_REGION }}" \
            --image "linode/arch" \
            --label "$SERVER_LABEL" \
            --root_pass "${{ secrets.SERVICE_ROOT_PASSWORD }}" \
            --authorized_keys "$SSH_PUBLIC_KEY" \
            --backups_enabled=${{ inputs.enable_backups || 'false' }} \
            --text --no-headers)
          
          SERVER_ID=$(echo "$RESULT" | cut -f1)
          
          # Wait for server to be running
          echo "⏳ Waiting for server to be ready..."
          ATTEMPT=0
          while true; do
            # Get server info and check status
            SERVER_INFO=$(linode-cli linodes view "$SERVER_ID" --text --no-headers)
            
            # Debug: show the full output on first few attempts
            if [[ $ATTEMPT -lt 3 ]]; then
              echo "🔍 Debug - Server info columns:"
              echo "$SERVER_INFO"
            fi
            
            # Status is in column 6 (ID|Label|Region|Type|Image|Status|IP|Backups)
            STATUS=$(echo "$SERVER_INFO" | cut -f6)
            
            echo "Attempt $((++ATTEMPT)): Status='$STATUS'"
            
            # Check if server is running
            if [[ "$STATUS" == "running" ]]; then
              echo "✅ Server is running!"
              break
            fi
            
            # Don't wait forever for server status
            if [[ $ATTEMPT -gt 15 ]]; then
              echo "⚠️ Server status check timeout - proceeding to SSH test"
              break
            fi
            
            sleep 5  # Check more frequently
          done
          
          # Get server IP
          SERVER_INFO=$(linode-cli linodes view "$SERVER_ID" --text --no-headers)
          echo "🔍 Debug - Server view output:"
          echo "$SERVER_INFO"
          
          # Try different columns for IP address
          SERVER_IP_COL4=$(echo "$SERVER_INFO" | cut -f4)
          SERVER_IP_COL5=$(echo "$SERVER_INFO" | cut -f5)
          SERVER_IP_COL6=$(echo "$SERVER_INFO" | cut -f6)
          SERVER_IP_COL7=$(echo "$SERVER_INFO" | cut -f7)
          
          echo "IP candidates: Col4='$SERVER_IP_COL4', Col5='$SERVER_IP_COL5', Col6='$SERVER_IP_COL6', Col7='$SERVER_IP_COL7'"
          
          # Use the first valid IP address we find
          for IP in "$SERVER_IP_COL4" "$SERVER_IP_COL5" "$SERVER_IP_COL6" "$SERVER_IP_COL7"; do
            if [[ "$IP" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
              SERVER_IP="$IP"
              break
            fi
          done
          
          if [[ -z "$SERVER_IP" ]]; then
            echo "❌ Could not extract IP address from server info"
            exit 1
          fi
          
          echo "✅ Server ready: $SERVER_IP (ID: $SERVER_ID)"
          
          echo "server_ip=$SERVER_IP" >> $GITHUB_OUTPUT
          echo "server_id=$SERVER_ID" >> $GITHUB_OUTPUT

      - name: ⏳ Wait for SSH Access
        run: |
          echo "⏳ Waiting for SSH access to ${{ steps.create-server.outputs.server_ip }}..."
          
          SSH_READY=false
          
          # First, test basic connectivity
          echo "🔍 Testing basic connectivity to port 22..."
          for i in {1..10}; do
            if timeout 5 nc -zv ${{ steps.create-server.outputs.server_ip }} 22 2>/dev/null; then
              echo "✅ Port 22 is reachable on attempt $i"
              break
            fi
            echo "Port 22 not ready, waiting 10 seconds..."
            sleep 10
          done
          
          # Test SSH with detailed error output
          echo "🔑 Testing SSH connection with private key..."
          
          for i in {1..15}; do
            echo "Attempt $i/15: Testing SSH connection..."
            
            # Use the generated private key for authentication
            SSH_OUTPUT=$(timeout 10 ssh -i ~/.ssh/linode_deployment_key -v -o StrictHostKeyChecking=no -o ConnectTimeout=5 -o ConnectionAttempts=1 \
               root@${{ steps.create-server.outputs.server_ip }} "echo 'SSH ready'" 2>&1 || echo "SSH_FAILED")
            
            if echo "$SSH_OUTPUT" | grep -q "SSH ready"; then
              echo "✅ SSH ready after $i attempts"
              SSH_READY=true
              break
            else
              echo "SSH failed. Last few lines of output:"
              echo "$SSH_OUTPUT" | tail -3
            fi
            
            echo "Waiting 15 seconds before next attempt..."
            sleep 15
          done
          
          if [[ "$SSH_READY" != "true" ]]; then
            echo "❌ SSH failed to become ready after 30 attempts (5 minutes)"
            echo "🔍 Debugging SSH connection..."
            
            # Try to get more info about why SSH is failing
            echo "Testing basic connectivity..."
            timeout 5 nc -zv ${{ steps.create-server.outputs.server_ip }} 22 || echo "Port 22 not reachable"
            
            exit 1
          fi

      - name: 🏗️ Stage 1 - Pre-Reboot Setup
        id: stage1-setup
        run: |
          echo "🏗️ Stage 1: Pre-reboot foundation setup..."
          
          cat > stage1-setup.sh << 'EOF'
          #!/bin/bash
          set -euo pipefail
          
          echo "🔄 Updating system..."
          # Update package databases first
          pacman -Sy --noconfirm
          
          # Handle NVIDIA firmware conflicts with comprehensive approach
          echo "🔧 Removing conflicting NVIDIA firmware files..."
          rm -rf /usr/lib/firmware/nvidia/ad103* 2>/dev/null || true
          rm -rf /usr/lib/firmware/nvidia/ad104* 2>/dev/null || true  
          rm -rf /usr/lib/firmware/nvidia/ad106* 2>/dev/null || true
          rm -rf /usr/lib/firmware/nvidia/ad107* 2>/dev/null || true
          
          # Remove symlinks and directories that cause conflicts
          find /usr/lib/firmware/nvidia -type l -delete 2>/dev/null || true
          find /usr/lib/firmware/nvidia -name "*.zst" -delete 2>/dev/null || true
          
          # Force upgrade with multiple overwrite patterns
          pacman -Su --noconfirm --overwrite='*' --overwrite='/usr/lib/firmware/nvidia/*' || {
            echo "🔄 First upgrade failed, trying more aggressive approach..."
            # Remove entire nvidia firmware directory and try again
            rm -rf /usr/lib/firmware/nvidia || true
            # Also handle pacman database conflicts
            rm -f /var/lib/pacman/db.lck || true
            pacman -Su --noconfirm --overwrite='*' || {
              echo "🔄 Still failing, trying with force overwrite..."
              pacman -Su --noconfirm --overwrite='*' --force || true
            }
          }
          
          echo "📦 Installing core packages (excluding iptables and netdata for now)..."
          # Install everything except iptables-related packages to avoid conflicts
          # Note: We'll install Netdata separately with cloud support
          pacman -S --noconfirm curl wget git docker docker-compose \
            tailscale fail2ban sudo base-devel
          
          echo "📊 Installing Netdata with cloud support..."
          # Install Netdata using official installer which includes claim script
          bash <(curl -Ss https://my-netdata.io/kickstart.sh) --dont-wait --disable-telemetry || {
            echo "⚠️ Official Netdata installer failed, falling back to package manager..."
            pacman -S --noconfirm netdata
          }
          
          echo "👥 Creating users..."
          useradd -m -s /bin/bash jordan || true
          echo "jordan:${{ secrets.JORDAN_PASSWORD }}" | chpasswd
          usermod -aG wheel,docker jordan
          
          useradd -m -s /bin/bash actions_user || true
          echo "actions_user:${{ secrets.ACTIONS_USER_PASSWORD }}" | chpasswd
          usermod -aG wheel,docker actions_user
          
          useradd -m -s /bin/bash ${{ env.SERVICE_NAME }}_user || true
          usermod -aG docker ${{ env.SERVICE_NAME }}_user
          # Set a password for SSH access (temporary, will be secured via Tailscale)
          echo "${{ env.SERVICE_NAME }}_user:${{ secrets.ACTIONS_USER_PASSWORD }}" | chpasswd
          
          echo "⚙️ Enabling services for post-reboot..."
          systemctl enable docker
          systemctl enable tailscaled
          systemctl enable netdata
          
          echo "� Creating post-reboot script..."
          cat > /usr/local/bin/stage2-post-reboot.sh << 'STAGE2EOF'
          #!/bin/bash
          set -euo pipefail
          
          echo "🚀 Stage 2: Post-reboot setup starting..."
          
          echo "🐳 Starting Docker..."
          
          echo "📦 Installing firewall packages after reboot..."
          # Install iptables and ufw after kernel update and reboot to avoid conflicts
          pacman -S --noconfirm ufw iptables-nft || {
            echo "🔄 Resolving iptables conflicts..."
            # Remove conflicting iptables package first
            pacman -Rdd --noconfirm iptables 2>/dev/null || true
            pacman -S --noconfirm iptables-nft ufw
          }
          
          echo "🐳 Starting Docker service..."
          systemctl start docker
          
          echo "🔗 Starting and authenticating Tailscale..."
          systemctl start tailscaled
          sleep 5
          # Set hostname to service name for better identification
          hostnamectl set-hostname "${{ env.SERVICE_NAME }}"
          tailscale up --authkey="${{ secrets.TAILSCALE_AUTH_KEY }}" --hostname="${{ env.SERVICE_NAME }}" --accept-routes --timeout=120s
          
          # Wait for Tailscale to be ready
          echo "⏳ Waiting for Tailscale connection..."
          for i in {1..30}; do
            if tailscale status | grep -q "Logged in"; then
              echo "✅ Tailscale connected successfully"
              break
            fi
            echo "Attempt $i/30: Waiting for Tailscale..."
            sleep 10
          done
          
          TAILSCALE_IP=$(tailscale ip -4 2>/dev/null || echo "pending")
          echo "🔗 Tailscale IP: $TAILSCALE_IP"
          echo "$TAILSCALE_IP" > /tmp/tailscale_ip
          
          echo "🔥 Configuring firewall after reboot..."
          ufw --force reset
          ufw default deny incoming
          ufw default allow outgoing
          ufw allow ssh
          ufw allow in on tailscale0
          ufw --force enable
          
          echo "🔐 Configuring SSH for service access..."
          # Allow password authentication for Tailscale network only
          sed -i 's/#PasswordAuthentication yes/PasswordAuthentication yes/' /etc/ssh/sshd_config
          sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_config
          # Restart SSH service
          systemctl restart sshd
          
          echo "� Starting monitoring..."
          systemctl start netdata
          
          echo "✅ Stage 2 complete - server ready for service deployment"
          STAGE2EOF
          
          chmod +x /usr/local/bin/stage2-post-reboot.sh
          
          echo "🔄 Creating systemd service for post-reboot setup..."
          cat > /etc/systemd/system/stage2-setup.service << 'SERVICEEOF'
          [Unit]
          Description=Stage 2 Post-Reboot Setup
          After=network-online.target
          Wants=network-online.target
          
          [Service]
          Type=oneshot
          ExecStart=/usr/local/bin/stage2-post-reboot.sh
          RemainAfterExit=yes
          StandardOutput=journal
          StandardError=journal
          
          [Install]
          WantedBy=multi-user.target
          SERVICEEOF
          
          systemctl enable stage2-setup.service
          
          echo "✅ Stage 1 complete - system ready for reboot"
          echo "NEEDS_REBOOT" > /tmp/stage1_status
          EOF
          
          scp -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no stage1-setup.sh root@${{ steps.create-server.outputs.server_ip }}:/tmp/
          ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} "chmod +x /tmp/stage1-setup.sh && /tmp/stage1-setup.sh"
          
          STAGE1_STATUS=$(ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} "cat /tmp/stage1_status" || echo "unknown")
          echo "stage1_status=$STAGE1_STATUS" >> $GITHUB_OUTPUT

      - name: 🔄 Reboot Server for Kernel Updates
        if: steps.stage1-setup.outputs.stage1_status == 'NEEDS_REBOOT'
        run: |
          echo "🔄 Rebooting server for kernel updates and service initialization..."
          ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} "reboot" || true
          
          echo "⏳ Waiting for server to come back online..."
          sleep 30
          
          # Wait for SSH to be available again
          for i in {1..20}; do
            if ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no -o ConnectTimeout=10 root@${{ steps.create-server.outputs.server_ip }} "echo 'SSH ready after reboot'"; then
              echo "✅ Server is back online after reboot"
              break
            fi
            echo "Attempt $i/20: Waiting for server to come back online..."
            sleep 15
          done

      - name: 🏗️ Stage 2 - Post-Reboot Verification
        id: stage2-setup
        run: |
          echo "🏗️ Stage 2: Verifying post-reboot setup..."
          
          # Debug: Check service status first
          echo "🔍 Checking stage2-setup service status..."
          ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} "systemctl status stage2-setup.service" || true
          
          # Wait for and verify stage2 service completion (check for success/completed status)
          echo "⏳ Waiting for stage2-setup service to complete..."
          STAGE2_COMPLETED=false
          for i in {1..15}; do
            SERVICE_STATUS=$(ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} "systemctl is-active stage2-setup.service" || echo "inactive")
            SERVICE_EXIT_STATUS=$(ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} "systemctl show stage2-setup.service --property=ExecMainStatus --value" || echo "unknown")
            
            echo "Attempt $i/15: Service status='$SERVICE_STATUS', Exit status='$SERVICE_EXIT_STATUS'"
            
            # For oneshot services, we want to check if it exited successfully (inactive but with successful exit)
            if [[ "$SERVICE_STATUS" == "inactive" && "$SERVICE_EXIT_STATUS" == "0" ]] || [[ "$SERVICE_STATUS" == "active" ]]; then
              echo "✅ Stage 2 setup service completed successfully"
              STAGE2_COMPLETED=true
              break
            fi
            
            # If it failed, show logs and continue anyway
            if [[ "$SERVICE_EXIT_STATUS" != "0" && "$SERVICE_EXIT_STATUS" != "unknown" && "$SERVICE_EXIT_STATUS" != "" ]]; then
              echo "⚠️ Stage 2 service may have failed (exit status: $SERVICE_EXIT_STATUS), checking logs..."
              ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} "journalctl -u stage2-setup.service --no-pager -l" || true
              break
            fi
            
            sleep 10
          done
          
          # Even if service check failed, verify individual services are running
          echo "🔍 Verifying individual services..."
          
          # Check Docker
          if ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} "systemctl is-active docker" >/dev/null 2>&1; then
            echo "✅ Docker is active"
          else
            echo "⚠️ Docker is not active, trying to start it..."
            ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} "systemctl start docker"
          fi
          
          # Check Tailscale
          if ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} "systemctl is-active tailscaled" >/dev/null 2>&1; then
            echo "✅ Tailscale is active"
          else
            echo "⚠️ Tailscale is not active, checking status..."
            ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} "systemctl status tailscaled" || true
          fi
          
          # Check Netdata
          if ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} "systemctl is-active netdata" >/dev/null 2>&1; then
            echo "✅ Netdata is active"
          else
            echo "⚠️ Netdata is not active, checking status..."
            ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} "systemctl status netdata" || true
          fi
          
          # Get Tailscale IP
          TAILSCALE_IP=$(ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} "cat /tmp/tailscale_ip 2>/dev/null || tailscale ip -4 2>/dev/null || echo 'unknown'")
          echo "🔗 Tailscale IP: $TAILSCALE_IP"
          echo "tailscale_ip=$TAILSCALE_IP" >> $GITHUB_OUTPUT
          
          if [[ "$STAGE2_COMPLETED" == "true" ]]; then
            echo "✅ Stage 2 verification complete - server ready for service deployment"
          else
            echo "⚠️ Stage 2 service check incomplete, but proceeding with deployment (services may still be working)"
          fi
          
          echo "✅ Stage 2 verification complete - server ready for service deployment"

  # ============================================================================
  # Service Deployment
  # ============================================================================
  deploy-service:
    name: 🚢 Deploy Service
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: [preflight-checks, setup-infrastructure]
    if: needs.preflight-checks.outputs.should_deploy == 'true'
    
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4

      - name: 🚢 Deploy Service
        run: |
          echo "🚢 Deploying ${{ env.SERVICE_NAME }}..."
          
          SERVER_IP="${{ needs.setup-infrastructure.outputs.server_ip }}"
          
          # Create deployment script
          if [[ -f "services/${{ env.SERVICE_NAME }}/deploy.sh" ]]; then
            cp "services/${{ env.SERVICE_NAME }}/deploy.sh" deploy-script.sh
          else
            cat > deploy-script.sh << 'EOF'
          #!/bin/bash
          set -euo pipefail
          
          echo "🚢 Deploying ${{ env.SERVICE_NAME }}..."
          
          # Setup service directory
          SERVICE_DIR="/home/${{ env.SERVICE_NAME }}_user/${{ env.SERVICE_NAME }}"
          
          if [[ -d "$SERVICE_DIR" ]]; then
            echo "Updating existing service..."
            cd "$SERVICE_DIR"
            git pull origin main || git pull origin master || echo "Git pull failed, continuing..."
          else
            echo "Cloning service repository..."
            git clone "https://github.com/${{ github.repository_owner }}/${{ env.SERVICE_NAME }}.git" "$SERVICE_DIR" || {
              echo "Git clone failed, creating service directory manually..."
              mkdir -p "$SERVICE_DIR"
            }
            chown -R ${{ env.SERVICE_NAME }}_user:${{ env.SERVICE_NAME }}_user "$SERVICE_DIR"
          fi
          
          cd "$SERVICE_DIR"
          
          # Deploy based on what's available
          if [[ -f "docker-compose.yml" ]]; then
            echo "🐳 Starting Docker Compose services..."
            sudo -u ${{ env.SERVICE_NAME }}_user docker-compose down || true
            sudo -u ${{ env.SERVICE_NAME }}_user docker-compose pull || echo "Pull failed, using local images"
            sudo -u ${{ env.SERVICE_NAME }}_user docker-compose up -d
          elif [[ -f "start.sh" ]]; then
            echo "🚀 Running start script..."
            chmod +x start.sh
            sudo -u ${{ env.SERVICE_NAME }}_user ./start.sh
          else
            echo "⚠️ No deployment method found (docker-compose.yml or start.sh)"
          fi
          
          echo "✅ Service deployment complete"
          EOF
          fi
          
          # Run deployment
          scp -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no deploy-script.sh root@$SERVER_IP:/tmp/
          ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@$SERVER_IP "chmod +x /tmp/deploy-script.sh && /tmp/deploy-script.sh"

      - name: 🔍 Setup Monitoring
        if: inputs.enable_monitoring
        run: |
          echo "🔍 Setting up monitoring..."
          
          SERVER_IP="${{ needs.setup-infrastructure.outputs.server_ip }}"
          
          cat > monitoring-setup.sh << 'MONITORING_EOF'
          #!/bin/bash
          set -euo pipefail
          
          echo "🔍 Setting up Netdata monitoring..."
          
          # Start Netdata first
          systemctl enable netdata
          systemctl start netdata
          
          # Wait for Netdata to start
          sleep 10
          
          # Configure Netdata to bind to all interfaces
          NETDATA_CONF="/etc/netdata/netdata.conf"
          if [[ -f "$NETDATA_CONF" ]]; then
            echo "Configuring Netdata to allow external connections..."
            sed -i 's/^.*bind socket to IP.*=.*$/\tbind socket to IP = 0.0.0.0/' "$NETDATA_CONF"
          else
            echo "Creating basic Netdata configuration..."
            mkdir -p /etc/netdata
            echo "[global]" > "$NETDATA_CONF"
            echo "        bind socket to IP = 0.0.0.0" >> "$NETDATA_CONF"
            echo "        default port = 19999" >> "$NETDATA_CONF"
            echo "" >> "$NETDATA_CONF"
            echo "[web]" >> "$NETDATA_CONF"
            echo "        allow connections from = *" >> "$NETDATA_CONF"
          fi
          
          # Configure firewall
          echo "Opening firewall for Netdata on Tailscale..."
          ufw allow in on tailscale0 to any port 19999
          
          # Restart Netdata with new config
          systemctl restart netdata
          sleep 5
          
          # Test if Netdata is responding
          echo "Testing Netdata connectivity..."
          if curl -f http://localhost:19999/api/v1/info 2>/dev/null >/dev/null; then
            echo "✅ Netdata is responding on port 19999"
          else
            echo "⚠️ Netdata may not be responding on port 19999"
          fi
          
          # Claim to Netdata Cloud if credentials provided
          if [[ -n "${{ secrets.NETDATA_CLAIM_TOKEN }}" && -n "${{ secrets.NETDATA_CLAIM_ROOM }}" ]]; then
            echo "🔗 Claiming Netdata to cloud..."
            
            # Multiple methods to find and run claim script
            CLAIM_SCRIPT=""
            
            # Method 1: Check common locations for claim script
            for script_path in "/usr/sbin/netdata-claim.sh" "/opt/netdata/bin/netdata-claim.sh" "/usr/libexec/netdata/netdata-claim.sh" "/usr/lib/netdata/netdata-claim.sh"; do
              if [[ -f "$script_path" ]]; then
                CLAIM_SCRIPT="$script_path"
                echo "Found claim script at: $script_path"
                break
              fi
            done
            
            # Method 2: Search for the script
            if [[ -z "$CLAIM_SCRIPT" ]]; then
              CLAIM_SCRIPT=$(find /usr /opt -name "netdata-claim.sh" 2>/dev/null | head -1)
              if [[ -n "$CLAIM_SCRIPT" ]]; then
                echo "Found claim script via search: $CLAIM_SCRIPT"
              fi
            fi
            
            # Method 3: Check if it's in PATH
            if [[ -z "$CLAIM_SCRIPT" ]] && command -v netdata-claim.sh >/dev/null 2>&1; then
              CLAIM_SCRIPT="netdata-claim.sh"
              echo "Found claim script in PATH"
            fi
            
            # Method 4: Download claim script directly if not found
            if [[ -z "$CLAIM_SCRIPT" ]]; then
              echo "Claim script not found locally, downloading..."
              wget -O /tmp/netdata-claim.sh https://raw.githubusercontent.com/netdata/netdata/master/claim/netdata-claim.sh
              chmod +x /tmp/netdata-claim.sh
              CLAIM_SCRIPT="/tmp/netdata-claim.sh"
            fi
            
            # Attempt to claim
            if [[ -n "$CLAIM_SCRIPT" ]]; then
              echo "Attempting to claim with script: $CLAIM_SCRIPT"
              "$CLAIM_SCRIPT" -token="${{ secrets.NETDATA_CLAIM_TOKEN }}" \
                -rooms="${{ secrets.NETDATA_CLAIM_ROOM }}" \
                -url=https://app.netdata.cloud \
                -hostname="${{ env.SERVICE_NAME }}" || {
                echo "⚠️ Netdata cloud claim failed with script - trying alternative method..."
                
                # Alternative method: Manual claiming via API
                echo "Trying manual claim via Netdata API..."
                curl -X POST "http://localhost:19999/api/v1/registry" \
                  -H "Content-Type: application/json" \
                  -d '{
                    "action": "claim",
                    "token": "${{ secrets.NETDATA_CLAIM_TOKEN }}",
                    "rooms": "${{ secrets.NETDATA_CLAIM_ROOM }}",
                    "url": "https://app.netdata.cloud",
                    "hostname": "${{ env.SERVICE_NAME }}"
                  }' || echo "Manual claim also failed"
              }
              
              # Restart Netdata after claiming
              systemctl restart netdata
              sleep 5
              
              echo "✅ Netdata claim process completed"
            else
              echo "⚠️ Could not find or download claim script - skipping cloud integration"
            fi
          else
            echo "ℹ️ No Netdata cloud credentials provided - running locally only"
          fi
          
          echo "✅ Netdata monitoring setup complete"
          MONITORING_EOF
          
          scp -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no monitoring-setup.sh root@$SERVER_IP:/tmp/
          ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@$SERVER_IP "chmod +x /tmp/monitoring-setup.sh && /tmp/monitoring-setup.sh"

  # ============================================================================
  # Health Checks
  # ============================================================================
  health-check:
    name: 🏥 Health Check
    runs-on: ubuntu-latest
    needs: [preflight-checks, setup-infrastructure, deploy-service]
    if: always() && (needs.deploy-service.result == 'success' || needs.preflight-checks.outputs.should_health_check == 'true')
    
    steps:
      - name: 🏥 Perform Health Checks
        run: |
          echo "🏥 Running health checks..."
          
          SERVER_IP="${{ needs.setup-infrastructure.outputs.server_ip }}"
          
          if [[ -n "$SERVER_IP" ]]; then
            echo "Testing SSH connectivity..."
            ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no -o ConnectTimeout=10 root@$SERVER_IP "echo 'SSH OK'"
            
            echo "Checking services..."
            ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@$SERVER_IP "systemctl is-active docker"
            ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@$SERVER_IP "tailscale status --peers=false"
            ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@$SERVER_IP "docker ps"
            
            echo "✅ Health checks passed"
          else
            echo "⚠️ No server IP available for health checks"
          fi

  # ============================================================================
  # Notifications
  # ============================================================================
  notify:
    name: 📢 Notify
    runs-on: ubuntu-latest
    needs: [preflight-checks, deploy-service, destroy-service, health-check]
    if: always()
    
    steps:
      - name: 📢 Send Notification
        env:
          DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK }}
        if: env.DISCORD_WEBHOOK != ''
        run: |
          echo "📢 Sending notification..."
          
          # Determine overall status
          if [[ "${{ needs.deploy-service.result }}" == "success" || "${{ needs.destroy-service.result }}" == "success" ]]; then
            STATUS="✅ SUCCESS"
            COLOR="3066993"
          else
            STATUS="❌ FAILED"
            COLOR="15158332"
          fi
          
          ACTION_EMOJI=""
          case "${{ env.ACTION_TYPE }}" in
            "deploy") ACTION_EMOJI="🚀" ;;
            "destroy") ACTION_EMOJI="🗑️" ;;
            "health-check") ACTION_EMOJI="🏥" ;;
            "restart") ACTION_EMOJI="🔄" ;;
          esac
          
          curl -H "Content-Type: application/json" \
            -d "{
              \"embeds\": [{
                \"title\": \"$ACTION_EMOJI $STATUS: ${{ env.SERVICE_NAME }} ${{ env.ACTION_TYPE }}\",
                \"description\": \"Service: ${{ env.SERVICE_NAME }}\\nAction: ${{ env.ACTION_TYPE }}\\nMode: ${{ env.DEPLOYMENT_MODE }}\\nTests Skipped: ${{ env.SKIP_TESTS }}\\nDocker Build Skipped: ${{ env.SKIP_DOCKER_BUILD }}\\nServer Overwritten: ${{ env.OVERWRITE_SERVER }}\",
                \"color\": $COLOR,
                \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%S.000Z)\"
              }]
            }" \
            "$DISCORD_WEBHOOK"
