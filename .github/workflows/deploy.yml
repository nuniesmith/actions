name: 🚀 Unified Service Management

# Docker Network Configuration:
# This workflow sets up dedicated Docker networks with static IP ranges for each service:
# - FKS:   172.20.0.0/16 (containers: 172.20.1.0/24)
# - ATS:   172.21.0.0/16 (containers: 172.21.1.0/24)  
# - Nginx: 172.22.0.0/16 (containers: 172.22.1.0/24)
# 
# Benefits:
# - Predictable IP addresses for service communication
# - Proper iptables rules for inter-service traffic
# - Tailscale subnet advertisement for VPN access to all containers
# - Network isolation between services while allowing controlled communication

on:
  workflow_call:
    inputs:
      service_name:
        description: 'Name of the service to manage (e.g., fks, nginx, ats)'
        required: true
        type: string
      
      action_type:
        description: 'Action to perform'
        required: false
        type: string
        default: 'deploy'
        # Options: deploy, destroy, health-check, restart
      
      deployment_mode:
        description: 'Deployment mode'
        required: false
        type: string
        default: 'full-deploy'
        # Options: full-deploy, update-only, restart-only, code-only
      
      # 🎯 New Options You Requested
      skip_tests:
        description: 'Skip running code tests'
        required: false
        type: boolean
        default: false
      
      skip_docker_build:
        description: 'Skip building Docker images'
        required: false
        type: boolean
        default: false
      
      build_docker_on_changes:
        description: 'Only build Docker if code/Dockerfile changed'
        required: false
        type: boolean
        default: true
      
      overwrite_server:
        description: 'Destroy and recreate Linode server'
        required: false
        type: boolean
        default: false
      
      # Server Configuration
      server_type:
        description: 'Linode server type'
        required: false
        type: string
        default: 'g6-standard-2'
      
      target_region:
        description: 'Linode region'
        required: false
        type: string
        default: 'ca-central'
      
      domain_suffix:
        description: 'Domain suffix (e.g., 7gram.xyz)'
        required: false
        type: string
        default: '7gram.xyz'
      
      custom_domains:
        description: 'Comma-separated list of custom domains for DNS updates (overrides defaults)'
        required: false
        type: string
        default: ''
      
      # Feature Toggles
      enable_backups:
        description: 'Enable Linode backups'
        required: false
        type: boolean
        default: false
      
      # Destroy Options (for destroy action)
      destroy_scope:
        description: 'What to destroy (for destroy action)'
        required: false
        type: string
        default: 'service-only'
      
      confirm_destruction:
        description: 'Type "DESTROY" to confirm destruction'
        required: false
        type: string

    secrets:
      # Core Infrastructure
      LINODE_CLI_TOKEN:
        required: true
      SERVICE_ROOT_PASSWORD:
        required: true
      
      # User Management
      JORDAN_PASSWORD:
        required: true
      ACTIONS_USER_PASSWORD:
        required: true
      
      # VPN & Networking
      TAILSCALE_AUTH_KEY:
        required: true
      TAILSCALE_TAILNET:
        description: 'Tailscale tailnet name (optional)'
        required: false
      
      # DNS Management (Optional)
      CLOUDFLARE_API_TOKEN:
        required: false
      CLOUDFLARE_ZONE_ID:
        required: false
      
      # Container Registry (Optional)
      DOCKER_USERNAME:
        required: false
      DOCKER_TOKEN:
        required: false
      
      # Notifications (Optional)
      DISCORD_WEBHOOK:
        required: false

    outputs:
      # Infrastructure outputs
      server_ip:
        description: 'Public IP address of the deployed server'
        value: ${{ jobs.setup-infrastructure.outputs.server_ip }}
      server_id:
        description: 'Linode server ID'
        value: ${{ jobs.setup-infrastructure.outputs.server_id }}
      tailscale_ip:
        description: 'Tailscale IP address of the server'
        value: ${{ jobs.setup-infrastructure.outputs.tailscale_ip }}

  workflow_dispatch:
    inputs:
      service_name:
        description: 'Name of the service to manage'
        required: true
        type: choice
        options:
          - 'fks'
          - 'nginx'
          - 'ats'
          - 'custom'
      
      action_type:
        description: 'Action to perform'
        required: true
        type: choice
        options:
          - 'deploy'
          - 'destroy'
          - 'health-check'
          - 'restart'
        default: 'deploy'
      
      deployment_mode:
        description: 'Deployment mode (for deploy action)'
        required: false
        type: choice
        options:
          - 'full-deploy'
          - 'update-only'
          - 'restart-only'
          - 'code-only'
        default: 'full-deploy'
      
      # 🎯 Your Requested Options
      skip_tests:
        description: 'Skip running code tests'
        required: false
        type: boolean
        default: false
      
      skip_docker_build:
        description: 'Skip building Docker images'
        required: false
        type: boolean
        default: false
      
      build_docker_on_changes:
        description: 'Only build Docker if code/Dockerfile changed'
        required: false
        type: boolean
        default: true
      
      overwrite_server:
        description: 'Destroy and recreate Linode server'
        required: false
        type: boolean
        default: false
      
      # Destroy Options (for destroy action)
      destroy_scope:
        description: 'What to destroy (for destroy action)'
        required: false
        type: choice
        options:
          - 'service-only'
          - 'full-server'
          - 'reset-service'
        default: 'service-only'
      
      confirm_destruction:
        description: 'Type "DESTROY" to confirm destruction'
        required: false
        type: string
      
      # Server Configuration
      server_type:
        description: 'Linode server type'
        required: false
        type: choice
        options:
          - 'g6-nanode-1'          # 1GB RAM
          - 'g6-standard-1'        # 2GB RAM
          - 'g6-standard-2'        # 4GB RAM
          - 'g6-standard-4'        # 8GB RAM
          - 'g6-standard-8'        # 16GB RAM
        default: 'g6-standard-2'

env:
  SERVICE_NAME: ${{ inputs.service_name }}
  ACTION_TYPE: ${{ inputs.action_type }}
  DEPLOYMENT_MODE: ${{ inputs.deployment_mode }}
  SERVER_TYPE: ${{ inputs.server_type }}
  TARGET_REGION: ${{ inputs.target_region || 'ca-central' }}
  DOMAIN_SUFFIX: ${{ inputs.domain_suffix || '7gram.xyz' }}
  FULL_DOMAIN: ${{ inputs.service_name }}.${{ inputs.domain_suffix || '7gram.xyz' }}
  
  # Your requested options
  SKIP_TESTS: ${{ inputs.skip_tests }}
  SKIP_DOCKER_BUILD: ${{ inputs.skip_docker_build }}
  BUILD_DOCKER_ON_CHANGES: ${{ inputs.build_docker_on_changes }}
  OVERWRITE_SERVER: ${{ inputs.overwrite_server }}

jobs:
  # ============================================================================
  # Pre-flight Checks & Validation
  # ============================================================================
  preflight-checks:
    name: 🛫 Pre-flight Checks
    runs-on: ubuntu-latest
    outputs:
      action_validated: ${{ steps.validate-action.outputs.validated }}
      should_destroy: ${{ steps.validate-action.outputs.should_destroy }}
      should_deploy: ${{ steps.validate-action.outputs.should_deploy }}
      should_health_check: ${{ steps.validate-action.outputs.should_health_check }}
      should_overwrite_server: ${{ steps.validate-action.outputs.should_overwrite_server }}
      destroy_confirmed: ${{ steps.validate-destroy.outputs.confirmed }}
      code_changed: ${{ steps.check-changes.outputs.code_changed }}
      docker_build_needed: ${{ steps.check-changes.outputs.docker_build_needed }}
    
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch full history for change detection

      - name: 🎯 Validate Action Type
        id: validate-action
        run: |
          echo "🎯 Validating action: ${{ env.ACTION_TYPE }}"
          
          case "${{ env.ACTION_TYPE }}" in
            "deploy")
              echo "should_deploy=true" >> $GITHUB_OUTPUT
              echo "should_destroy=false" >> $GITHUB_OUTPUT
              echo "should_health_check=false" >> $GITHUB_OUTPUT
              ;;
            "destroy")
              echo "should_deploy=false" >> $GITHUB_OUTPUT
              echo "should_destroy=true" >> $GITHUB_OUTPUT
              echo "should_health_check=false" >> $GITHUB_OUTPUT
              ;;
            "health-check")
              echo "should_deploy=false" >> $GITHUB_OUTPUT
              echo "should_destroy=false" >> $GITHUB_OUTPUT
              echo "should_health_check=true" >> $GITHUB_OUTPUT
              ;;
            "restart")
              echo "should_deploy=true" >> $GITHUB_OUTPUT
              echo "should_destroy=false" >> $GITHUB_OUTPUT
              echo "should_health_check=false" >> $GITHUB_OUTPUT
              ;;
            *)
              echo "❌ Invalid action type: ${{ env.ACTION_TYPE }}"
              exit 1
              ;;
          esac
          
          # Check if server should be overwritten
          if [[ "${{ env.OVERWRITE_SERVER }}" == "true" && "${{ env.ACTION_TYPE }}" == "deploy" ]]; then
            echo "should_overwrite_server=true" >> $GITHUB_OUTPUT
            echo "⚠️ Server will be overwritten (destroyed and recreated)"
          else
            echo "should_overwrite_server=false" >> $GITHUB_OUTPUT
          fi
          
          echo "validated=true" >> $GITHUB_OUTPUT

      - name: ⚠️ Validate Destruction Request
        id: validate-destroy
        if: steps.validate-action.outputs.should_destroy == 'true' || steps.validate-action.outputs.should_overwrite_server == 'true'
        run: |
          # For server overwrite during deployment, skip confirmation requirement
          if [[ "${{ steps.validate-action.outputs.should_overwrite_server }}" == "true" && "${{ env.ACTION_TYPE }}" == "deploy" ]]; then
            echo "✅ Server overwrite confirmed (deploy mode)"
            echo "confirmed=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # For explicit destroy actions, require confirmation
          if [[ "${{ inputs.confirm_destruction }}" != "DESTROY" ]]; then
            echo "❌ Destruction not confirmed. You must type 'DESTROY' exactly."
            echo "confirmed=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          echo "✅ Destruction confirmed for ${{ env.SERVICE_NAME }}"
          echo "confirmed=true" >> $GITHUB_OUTPUT

      - name: 🔍 Check for Code Changes
        id: check-changes
        if: steps.validate-action.outputs.should_deploy == 'true'
        run: |
          echo "🔍 Checking for code and Docker changes..."
          
          # TEMPORARY: Force Docker builds for all services since DockerHub images were cleared
          echo "🔄 FORCING Docker builds - DockerHub images were cleared"
          echo "code_changed=true" >> $GITHUB_OUTPUT
          echo "docker_build_needed=true" >> $GITHUB_OUTPUT
          
          # TODO: Re-enable change detection later by uncommenting the logic below
          # and removing the forced build logic above
          
          # Check if this is the first commit or if we should build anyway
          #if [[ $(git rev-list --count HEAD) -le 1 ]] || [[ "${{ env.BUILD_DOCKER_ON_CHANGES }}" == "false" ]]; then
          #  echo "First commit or change detection disabled - assuming changes exist"
          #  echo "code_changed=true" >> $GITHUB_OUTPUT
          #  echo "docker_build_needed=true" >> $GITHUB_OUTPUT
          #else
          #  # Check for changes in the last commit
          #  CHANGED_FILES=$(git diff --name-only HEAD~1 HEAD)
          #  echo "Changed files: $CHANGED_FILES"
          #  
          #  # Check if code files changed (exclude docs, configs, etc.)
          #  CODE_CHANGED="false"
          #  if echo "$CHANGED_FILES" | grep -E '\.(js|ts|py|go|java|cpp|c|rs|php)$' > /dev/null; then
          #    CODE_CHANGED="true"
          #    echo "✅ Code files changed"
          #  fi
          #  
          #  # Check if Docker-related files changed
          #  DOCKER_BUILD_NEEDED="false"
          #  if echo "$CHANGED_FILES" | grep -E '(Dockerfile|docker-compose|requirements|package\.json|go\.mod|Cargo\.toml)' > /dev/null; then
          #    DOCKER_BUILD_NEEDED="true"
          #    echo "✅ Docker-related files changed"
          #  fi
          #  
          #  # If build_docker_on_changes is true, only build if changes detected
          #  if [[ "${{ env.BUILD_DOCKER_ON_CHANGES }}" == "true" ]]; then
          #    if [[ "$CODE_CHANGED" == "true" || "$DOCKER_BUILD_NEEDED" == "true" ]]; then
          #      DOCKER_BUILD_NEEDED="true"
          #    else
          #      DOCKER_BUILD_NEEDED="false"
          #      echo "ℹ️ No relevant changes detected - skipping Docker build"
          #    fi
          #  fi
          #  
          #  echo "code_changed=$CODE_CHANGED" >> $GITHUB_OUTPUT
          #  echo "docker_build_needed=$DOCKER_BUILD_NEEDED" >> $GITHUB_OUTPUT
          #fi

      - name: 🔐 Validate Secrets
        env:
          LINODE_CLI_TOKEN: ${{ secrets.LINODE_CLI_TOKEN }}
          SERVICE_ROOT_PASSWORD: ${{ secrets.SERVICE_ROOT_PASSWORD }}
          JORDAN_PASSWORD: ${{ secrets.JORDAN_PASSWORD }}
          ACTIONS_USER_PASSWORD: ${{ secrets.ACTIONS_USER_PASSWORD }}
          TAILSCALE_AUTH_KEY: ${{ secrets.TAILSCALE_AUTH_KEY }}
          # SSL-related secrets (optional for nginx)
          CLOUDFLARE_EMAIL: ${{ secrets.CLOUDFLARE_EMAIL }}
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
        run: |
          echo "🔐 Validating required secrets..."
          
          MISSING_SECRETS=()
          [[ -z "$LINODE_CLI_TOKEN" ]] && MISSING_SECRETS+=("LINODE_CLI_TOKEN")
          [[ -z "$SERVICE_ROOT_PASSWORD" ]] && MISSING_SECRETS+=("SERVICE_ROOT_PASSWORD")
          [[ -z "$JORDAN_PASSWORD" ]] && MISSING_SECRETS+=("JORDAN_PASSWORD")
          [[ -z "$ACTIONS_USER_PASSWORD" ]] && MISSING_SECRETS+=("ACTIONS_USER_PASSWORD")
          [[ -z "$TAILSCALE_AUTH_KEY" ]] && MISSING_SECRETS+=("TAILSCALE_AUTH_KEY")
          
          if [[ ${#MISSING_SECRETS[@]} -gt 0 ]]; then
            echo "❌ Missing required secrets:"
            printf '  - %s\n' "${MISSING_SECRETS[@]}"
            exit 1
          fi
          
          echo "✅ All required secrets validated"
          
          # Check SSL-related secrets for nginx deployments
          if [[ "${{ env.SERVICE_NAME }}" == "nginx" ]]; then
            SSL_SECRETS=()
            [[ -z "$CLOUDFLARE_EMAIL" ]] && SSL_SECRETS+=("CLOUDFLARE_EMAIL")
            [[ -z "$CLOUDFLARE_API_TOKEN" ]] && SSL_SECRETS+=("CLOUDFLARE_API_TOKEN")
            
            if [[ ${#SSL_SECRETS[@]} -gt 0 ]]; then
              echo "⚠️ SSL secrets missing (will use HTTP challenge or self-signed):"
              printf '  - %s\n' "${SSL_SECRETS[@]}"
              echo "💡 For wildcard certificates and better reliability, add:"
              echo "   - CLOUDFLARE_EMAIL (your Cloudflare account email)"
              echo "   - CLOUDFLARE_API_TOKEN (Cloudflare API token with DNS edit permissions)"
            else
              echo "✅ SSL secrets available for DNS-01 challenge"
            fi
          fi

  # ============================================================================
  # Resource Cleanup (Optional, runs before deployment)
  # ============================================================================
  cleanup-old-resources:
    name: 🧹 Cleanup Old Resources
    runs-on: ubuntu-latest
    needs: preflight-checks
    if: |
      needs.preflight-checks.outputs.should_deploy == 'true' && 
      (github.event_name == 'push' || inputs.overwrite_server == true)
    
    steps:
      - name: 🧹 Cleanup Tailscale and Linode Resources
        env:
          LINODE_CLI_TOKEN: ${{ secrets.LINODE_CLI_TOKEN }}
          TAILSCALE_AUTH_KEY: ${{ secrets.TAILSCALE_AUTH_KEY }}
          TAILSCALE_TAILNET: ${{ secrets.TAILSCALE_TAILNET }}
        run: |
          echo "🧹 Cleaning up old ${{ env.SERVICE_NAME }} resources..."
          
          # Install dependencies
          sudo apt-get update && sudo apt-get install -y curl jq
          
          # Cleanup Tailscale devices first
          if [[ -n "$TAILSCALE_AUTH_KEY" && -n "$TAILSCALE_TAILNET" ]]; then
            echo "🔗 Cleaning up old Tailscale devices..."
            
            # Remove old service devices using direct API
            echo "🔍 Comprehensive cleanup of old ${{ env.SERVICE_NAME }} devices..."
            DEVICES_RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" -H "Authorization: Bearer $TAILSCALE_AUTH_KEY" \
              "https://api.tailscale.com/api/v2/tailnet/$TAILSCALE_TAILNET/devices" 2>/dev/null || echo "CURL_FAILED")
            
            # Extract HTTP status code
            HTTP_CODE=$(echo "$DEVICES_RESPONSE" | grep "HTTP_CODE:" | cut -d: -f2)
            DEVICES_JSON=$(echo "$DEVICES_RESPONSE" | sed '/HTTP_CODE:/d')
            
            if [[ "$HTTP_CODE" == "200" ]] && echo "$DEVICES_JSON" | jq -e '.devices' >/dev/null 2>&1; then
              # Multiple patterns to catch all service variants
              CLEANUP_PATTERNS=(
                "^${{ env.SERVICE_NAME }}(-[0-9]+)?$"
                "^${{ env.SERVICE_NAME }}$" 
                "^${{ env.SERVICE_NAME }}-"
                "${{ env.SERVICE_NAME }}"
              )
              
              TOTAL_REMOVED=0
              ALL_FOUND_DEVICES=""
              
              # Collect all matching device IDs
              for pattern in "${CLEANUP_PATTERNS[@]}"; do
                echo "🔍 Searching for pattern: $pattern"
                
                DEVICES=$(echo "$DEVICES_JSON" | jq -r --arg pattern "$pattern" '
                  .devices[]? | 
                  select(
                    (.hostname // .name | test($pattern; "i")) or 
                    (.name // .hostname | test($pattern; "i"))
                  ) | 
                  .id // empty' | grep -v '^$')
                
                if [[ -n "$DEVICES" ]]; then
                  ALL_FOUND_DEVICES="$ALL_FOUND_DEVICES $DEVICES"
                fi
              done
              
              # Remove duplicates
              UNIQUE_DEVICES=$(echo "$ALL_FOUND_DEVICES" | tr ' ' '\n' | sort -u | grep -v '^$' || true)
              
              if [[ -n "$UNIQUE_DEVICES" ]]; then
                echo "Found ${{ env.SERVICE_NAME }} devices to remove..."
                for device_id in $UNIQUE_DEVICES; do
                  if [[ -n "$device_id" ]]; then
                    DEVICE_INFO=$(echo "$DEVICES_JSON" | jq -r --arg id "$device_id" '.devices[]? | select(.id == $id) | "\(.hostname // .name // "unknown") (\(.addresses[0] // "no-ip"))"')
                    echo "🗑️ Removing Tailscale device: $device_id - $DEVICE_INFO"
                    
                    # Try primary endpoint
                    REMOVE_RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" -X DELETE \
                      -H "Authorization: Bearer $TAILSCALE_AUTH_KEY" \
                      "https://api.tailscale.com/api/v2/device/$device_id" 2>/dev/null || echo "CURL_FAILED")
                    
                    REMOVE_CODE=$(echo "$REMOVE_RESPONSE" | grep "HTTP_CODE:" | cut -d: -f2)
                    
                    if [[ "$REMOVE_CODE" == "200" || "$REMOVE_CODE" == "204" ]]; then
                      echo "   ✅ Successfully removed"
                      ((TOTAL_REMOVED++))
                    else
                      echo "   ⚠️ Trying alternative endpoint..."
                      # Try alternative endpoint
                      ALT_REMOVE_RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" -X DELETE \
                        -H "Authorization: Bearer $TAILSCALE_AUTH_KEY" \
                        "https://api.tailscale.com/api/v2/tailnet/$TAILSCALE_TAILNET/devices/$device_id" 2>/dev/null || echo "CURL_FAILED")
                      
                      ALT_REMOVE_CODE=$(echo "$ALT_REMOVE_RESPONSE" | grep "HTTP_CODE:" | cut -d: -f2)
                      if [[ "$ALT_REMOVE_CODE" == "200" || "$ALT_REMOVE_CODE" == "204" ]]; then
                        echo "   ✅ Successfully removed via alternative endpoint"
                        ((TOTAL_REMOVED++))
                      else
                        echo "   ❌ Failed to remove device $device_id"
                      fi
                    fi
                    sleep 1
                  fi
                done
                echo "✅ ${{ env.SERVICE_NAME }} Tailscale cleanup completed - removed $TOTAL_REMOVED devices"
              else
                echo "No ${{ env.SERVICE_NAME }} devices found to remove"
              fi
            else
              echo "⚠️ No devices array in Tailscale response or API error"
            fi
          else
            echo "⚠️ Tailscale API credentials not available"
          fi
          
          # Cleanup Linode servers
          if [[ -n "$LINODE_CLI_TOKEN" ]]; then
            echo "🖥️ Cleaning up old Linode servers..."
            
            # Install linode-cli
            pip install linode-cli
            
            # Configure linode-cli
            echo "[DEFAULT]
          token = $LINODE_CLI_TOKEN" > ~/.linode-cli
            
            # Remove old service servers
            echo "🔍 Looking for old ${{ env.SERVICE_NAME }} servers..."
            OLD_SERVERS=$(linode-cli linodes list --json | jq -r --arg service "${{ env.SERVICE_NAME }}" '.[] | select(.label | startswith($service)) | .id')
            
            for server_id in $OLD_SERVERS; do
              if [[ -n "$server_id" ]]; then
                echo "🗑️ Removing old Linode server: $server_id (${{ env.SERVICE_NAME }})"
                linode-cli linodes delete "$server_id" --json || echo "Failed to remove server $server_id"
                sleep 10
              fi
            done
          else
            echo "⚠️ Linode CLI token not available"
          fi
          
          echo "✅ Cleanup completed for ${{ env.SERVICE_NAME }}"

  # ============================================================================
  # Code Testing (Optional)
  # ============================================================================
  run-tests:
    name: 🧪 Run Tests
    runs-on: ubuntu-latest
    needs: [preflight-checks, cleanup-old-resources]
    if: |
      always() &&
      needs.preflight-checks.outputs.should_deploy == 'true' && 
      inputs.skip_tests == false &&
      (needs.cleanup-old-resources.result == 'success' || needs.cleanup-old-resources.result == 'skipped')
    
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4

      - name: 🧪 Auto-detect and Run Tests
        run: |
          echo "🧪 Auto-detecting test framework..."
          
          # Node.js/JavaScript tests
          if [[ -f "package.json" ]]; then
            echo "📦 Node.js project detected"
            if command -v npm &> /dev/null; then
              echo "Installing dependencies..."
              npm install
              
              if npm run test --if-present; then
                echo "✅ Node.js tests passed"
              else
                echo "⚠️ Node.js tests failed or no test script found"
              fi
            fi
          fi
          
          # Python tests
          if [[ -f "requirements.txt" ]] || [[ -f "pyproject.toml" ]] || [[ -f "setup.py" ]]; then
            echo "🐍 Python project detected"
            if command -v python3 &> /dev/null; then
              if [[ -f "requirements.txt" ]]; then
                pip install -r requirements.txt
              fi
              
              # Try different test runners
              if python -m pytest --version &> /dev/null && find . -name "*test*.py" | grep -q .; then
                echo "Running pytest..."
                python -m pytest
              elif python -m unittest discover -s . -p "*test*.py" 2>/dev/null; then
                echo "✅ Python unittest tests passed"
              else
                echo "ℹ️ No Python tests found or test framework not available"
              fi
            fi
          fi
          
          # Go tests
          if [[ -f "go.mod" ]]; then
            echo "🔷 Go project detected"
            if command -v go &> /dev/null; then
              go test ./...
              echo "✅ Go tests passed"
            fi
          fi
          
          echo "✅ Test phase complete"

  # ============================================================================
  # Docker Build (Conditional)
  # ============================================================================
  build-docker-api:
    name: 🐳 Build API Docker Images
    runs-on: ubuntu-latest
    needs: [preflight-checks, cleanup-old-resources, run-tests]
    if: |
      always() && 
      needs.preflight-checks.outputs.should_deploy == 'true' && 
      inputs.skip_docker_build == false && 
      needs.preflight-checks.outputs.docker_build_needed == 'true' &&
      (needs.cleanup-old-resources.result == 'success' || needs.cleanup-old-resources.result == 'skipped') &&
      (needs.run-tests.result == 'success' || needs.run-tests.result == 'skipped')
    
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4

      - name: 🔑 Login to Docker Hub
        env:
          DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
          DOCKER_TOKEN: ${{ secrets.DOCKER_TOKEN }}
        if: env.DOCKER_USERNAME != '' && env.DOCKER_TOKEN != ''
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_TOKEN }}

      - name: 🐳 Build and Push API Docker Images
        run: |
          echo "🐳 Building API Docker images for ${{ env.SERVICE_NAME }}..."
          
          # Build API services (Python servers: api, worker, data)
          API_SERVICES=()
          
          # Check for API service configuration
          if [[ -f "docker-compose.api.yml" ]]; then
            echo "📋 Found docker-compose.api.yml - building API services"
            docker compose -f docker-compose.api.yml build
            
            if [[ -n "${{ secrets.DOCKER_USERNAME }}" ]]; then
              echo "📤 Pushing API compose images..."
              docker compose -f docker-compose.api.yml push || echo "⚠️ Some API images may not have push configured"
            fi
          elif [[ -f "docker-compose.yml" ]]; then
            # Build only API-related services from main compose file
            echo "📋 Building API services from main docker-compose.yml"
            
            # Extract API service names (common patterns: api, worker, data, backend)
            API_SERVICES=($(docker compose config --services 2>/dev/null | grep -E '^(api|worker|data|backend|server)$' || true))
            
            if [[ ${#API_SERVICES[@]} -gt 0 ]]; then
              echo "🔍 Found API services: ${API_SERVICES[*]}"
              
              for service in "${API_SERVICES[@]}"; do
                echo "🐳 Building service: $service"
                docker compose build "$service"
                
                if [[ -n "${{ secrets.DOCKER_USERNAME }}" ]]; then
                  echo "📤 Pushing service: $service"
                  docker compose push "$service" || echo "⚠️ Failed to push $service"
                fi
              done
            else
              echo "ℹ️ No API services found in docker-compose.yml"
            fi
          elif [[ -f "Dockerfile" ]]; then
            # Single Dockerfile - assume it's for API if service name suggests it
            if [[ "${{ env.SERVICE_NAME }}" =~ (api|backend|server) ]]; then
              echo "📋 Found Dockerfile - building as API service"
              
              IMAGE_TAG="${{ secrets.DOCKER_USERNAME }}/${{ env.SERVICE_NAME }}-api:latest"
              
              docker build -t "$IMAGE_TAG" .
              
              if [[ -n "${{ secrets.DOCKER_USERNAME }}" ]]; then
                echo "📤 Pushing image: $IMAGE_TAG"
                docker push "$IMAGE_TAG"
              fi
            else
              echo "ℹ️ Service doesn't appear to be API-focused - skipping API build"
            fi
          else
            echo "ℹ️ No Docker configuration found for API services"
          fi
          
          echo "✅ API Docker build complete"

  build-docker-web:
    name: 🌐 Build Web Docker Images  
    runs-on: ubuntu-latest
    needs: [preflight-checks, cleanup-old-resources, run-tests]
    if: |
      always() && 
      needs.preflight-checks.outputs.should_deploy == 'true' && 
      inputs.skip_docker_build == false && 
      needs.preflight-checks.outputs.docker_build_needed == 'true' &&
      (needs.cleanup-old-resources.result == 'success' || needs.cleanup-old-resources.result == 'skipped') &&
      (needs.run-tests.result == 'success' || needs.run-tests.result == 'skipped')
    
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4

      - name: 🔑 Login to Docker Hub
        env:
          DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
          DOCKER_TOKEN: ${{ secrets.DOCKER_TOKEN }}
        if: env.DOCKER_USERNAME != '' && env.DOCKER_TOKEN != ''
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_TOKEN }}

      - name: 🌐 Build and Push Web Docker Images
        run: |
          echo "🌐 Building Web Docker images for ${{ env.SERVICE_NAME }}..."
          
          # Build Web services (React web interface, frontend)
          WEB_SERVICES=()
          
          # Check for Web service configuration
          if [[ -f "docker-compose.web.yml" ]]; then
            echo "📋 Found docker-compose.web.yml - building Web services"
            docker compose -f docker-compose.web.yml build
            
            if [[ -n "${{ secrets.DOCKER_USERNAME }}" ]]; then
              echo "📤 Pushing Web compose images..."
              docker compose -f docker-compose.web.yml push || echo "⚠️ Some Web images may not have push configured"
            fi
          elif [[ -f "docker-compose.yml" ]]; then
            # Build only Web-related services from main compose file
            echo "📋 Building Web services from main docker-compose.yml"
            
            # Extract Web service names (common patterns: web, frontend, ui, client)
            WEB_SERVICES=($(docker compose config --services 2>/dev/null | grep -E '^(web|frontend|ui|client|app)$' || true))
            
            if [[ ${#WEB_SERVICES[@]} -gt 0 ]]; then
              echo "🔍 Found Web services: ${WEB_SERVICES[*]}"
              
              for service in "${WEB_SERVICES[@]}"; do
                echo "🌐 Building service: $service"
                docker compose build "$service"
                
                if [[ -n "${{ secrets.DOCKER_USERNAME }}" ]]; then
                  echo "📤 Pushing service: $service"
                  docker compose push "$service" || echo "⚠️ Failed to push $service"
                fi
              done
            else
              echo "ℹ️ No Web services found in docker-compose.yml"
            fi
          elif [[ -f "Dockerfile" ]]; then
            # Single Dockerfile - assume it's for Web if service name suggests it
            if [[ "${{ env.SERVICE_NAME }}" =~ (web|frontend|ui|client) ]]; then
              echo "📋 Found Dockerfile - building as Web service"
              
              IMAGE_TAG="${{ secrets.DOCKER_USERNAME }}/${{ env.SERVICE_NAME }}-web:latest"
              
              docker build -t "$IMAGE_TAG" .
              
              if [[ -n "${{ secrets.DOCKER_USERNAME }}" ]]; then
                echo "📤 Pushing image: $IMAGE_TAG"
                docker push "$IMAGE_TAG"
              fi
            else
              echo "ℹ️ Service doesn't appear to be Web-focused - skipping Web build"
            fi
          else
            echo "ℹ️ No Docker configuration found for Web services"
          fi
          
          echo "✅ Web Docker build complete"

  build-docker-auth:
    name: 🔐 Build Auth Docker Images
    runs-on: ubuntu-latest
    needs: [preflight-checks, cleanup-old-resources, run-tests]
    if: |
      always() && 
      needs.preflight-checks.outputs.should_deploy == 'true' && 
      inputs.skip_docker_build == false && 
      needs.preflight-checks.outputs.docker_build_needed == 'true' &&
      (needs.cleanup-old-resources.result == 'success' || needs.cleanup-old-resources.result == 'skipped') &&
      (needs.run-tests.result == 'success' || needs.run-tests.result == 'skipped')
    
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4

      - name: 🔑 Login to Docker Hub
        env:
          DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
          DOCKER_TOKEN: ${{ secrets.DOCKER_TOKEN }}
        if: env.DOCKER_USERNAME != '' && env.DOCKER_TOKEN != ''
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_TOKEN }}

      - name: 🔐 Build and Push Auth Docker Images
        run: |
          echo "🔐 Building Auth Docker images for ${{ env.SERVICE_NAME }}..."
          
          # Build Auth services (Authentication, authorization services)
          AUTH_SERVICES=()
          
          # Check for Auth service configuration
          if [[ -f "docker-compose.auth.yml" ]]; then
            echo "📋 Found docker-compose.auth.yml - building Auth services"
            docker compose -f docker-compose.auth.yml build
            
            if [[ -n "${{ secrets.DOCKER_USERNAME }}" ]]; then
              echo "📤 Pushing Auth compose images..."
              docker compose -f docker-compose.auth.yml push || echo "⚠️ Some Auth images may not have push configured"
            fi
          elif [[ -f "docker-compose.yml" ]]; then
            # Build only Auth-related services from main compose file
            echo "📋 Building Auth services from main docker-compose.yml"
            
            # Extract Auth service names (common patterns: auth, oauth, keycloak, etc.)
            AUTH_SERVICES=($(docker compose config --services 2>/dev/null | grep -E '^(auth|oauth|keycloak|identity|session)$' || true))
            
            if [[ ${#AUTH_SERVICES[@]} -gt 0 ]]; then
              echo "🔍 Found Auth services: ${AUTH_SERVICES[*]}"
              
              for service in "${AUTH_SERVICES[@]}"; do
                echo "🔐 Building service: $service"
                docker compose build "$service"
                
                if [[ -n "${{ secrets.DOCKER_USERNAME }}" ]]; then
                  echo "📤 Pushing service: $service"
                  docker compose push "$service" || echo "⚠️ Failed to push $service"
                fi
              done
            else
              echo "ℹ️ No Auth services found in docker-compose.yml - may use external auth services"
            fi
          else
            echo "ℹ️ No custom Auth services to build - likely using external authentication"
          fi
          
          echo "✅ Auth Docker build complete (or skipped if no custom services)"

  # ============================================================================
  # Server Destruction (if overwrite requested)
  # ============================================================================
  destroy-existing-server:
    name: 💥 Destroy Existing Server
    runs-on: ubuntu-latest
    needs: [preflight-checks, cleanup-old-resources, build-docker-api, build-docker-web, build-docker-auth]
    if: |
      always() && 
      needs.preflight-checks.outputs.should_overwrite_server == 'true' &&
      needs.preflight-checks.outputs.destroy_confirmed == 'true' &&
      (needs.cleanup-old-resources.result == 'success' || needs.cleanup-old-resources.result == 'skipped') &&
      (needs.build-docker-api.result == 'success' || needs.build-docker-api.result == 'skipped') &&
      (needs.build-docker-web.result == 'success' || needs.build-docker-web.result == 'skipped') &&
      (needs.build-docker-auth.result == 'success' || needs.build-docker-auth.result == 'skipped')
    
    steps:
      - name: � Checkout repository
        uses: actions/checkout@v4

      - name: �🔧 Setup Linode CLI
        env:
          LINODE_CLI_TOKEN: ${{ secrets.LINODE_CLI_TOKEN }}
        run: |
          pip install linode-cli
          # Configure via environment variable to avoid config file issues
          export LINODE_CLI_TOKEN="${{ secrets.LINODE_CLI_TOKEN }}"
          # Test CLI access
          linode-cli --version

      - name: 🧹 Cleanup Services Before Destruction
        env:
          LINODE_CLI_TOKEN: ${{ secrets.LINODE_CLI_TOKEN }}
          TAILSCALE_AUTH_KEY: ${{ secrets.TAILSCALE_AUTH_KEY }}
          TAILSCALE_TAILNET: ${{ secrets.TAILSCALE_TAILNET }}
        run: |
          echo "🧹 Cleaning up external service registrations..."
          
          # Install dependencies
          sudo apt-get update && sudo apt-get install -y curl jq sshpass >/dev/null 2>&1 || true
          
          # Find existing server
          EXISTING_SERVER=$(linode-cli linodes list --text --no-headers | grep "${{ env.SERVICE_NAME }}" | head -1)
          
          if [[ -n "$EXISTING_SERVER" ]]; then
            SERVER_ID=$(echo "$EXISTING_SERVER" | cut -f1)
            SERVER_LABEL=$(echo "$EXISTING_SERVER" | cut -f2)
            
            # Try to get server IP for cleanup
            SERVER_INFO=$(linode-cli linodes view "$SERVER_ID" --text --no-headers)
            SERVER_IP_COL4=$(echo "$SERVER_INFO" | cut -f4)
            SERVER_IP_COL5=$(echo "$SERVER_INFO" | cut -f5)
            SERVER_IP_COL6=$(echo "$SERVER_INFO" | cut -f6)
            SERVER_IP_COL7=$(echo "$SERVER_INFO" | cut -f7)
            
            SERVER_IP=""
            for IP in "$SERVER_IP_COL4" "$SERVER_IP_COL5" "$SERVER_IP_COL6" "$SERVER_IP_COL7"; do
              if [[ "$IP" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
                SERVER_IP="$IP"
                break
              fi
            done
            
            if [[ -n "$SERVER_IP" ]]; then
              echo "🔍 Found server to cleanup: $SERVER_LABEL ($SERVER_IP)"
              
              # Try to connect using password authentication for cleanup
              echo "🔗 Attempting cleanup connection via password auth..."
              
              # Try to connect and cleanup using root password
              if timeout 30 sshpass -p "${{ secrets.SERVICE_ROOT_PASSWORD }}" ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 root@$SERVER_IP "echo 'Cleanup connection successful'" 2>/dev/null; then
                echo "🔗 Connected to server for cleanup..."
                
                # Cleanup Tailscale device
                echo "🧹 Removing Tailscale device from network..."
                sshpass -p "${{ secrets.SERVICE_ROOT_PASSWORD }}" ssh -o StrictHostKeyChecking=no root@$SERVER_IP "tailscale logout 2>/dev/null || true" || true
                
                # Get device info before cleanup
                DEVICE_NAME=$(sshpass -p "${{ secrets.SERVICE_ROOT_PASSWORD }}" ssh -o StrictHostKeyChecking=no root@$SERVER_IP "hostname 2>/dev/null || echo '${{ env.SERVICE_NAME }}'" || echo "${{ env.SERVICE_NAME }}")
                echo "Device name for cleanup: $DEVICE_NAME"
                
                echo "✅ Service cleanup completed"
              else
                echo "⚠️ Could not connect to server for cleanup (server may be down, password changed, or SSH not configured)"
                echo "ℹ️ This is normal if the server was already destroyed or is not responding"
              fi
            else
              echo "⚠️ Could not determine server IP for cleanup"
            fi
            
            # Cleanup Tailscale devices via API (more reliable and comprehensive)
            if [[ -n "$TAILSCALE_AUTH_KEY" ]]; then
              echo "🔗 Comprehensive Tailscale cleanup for service ${{ env.SERVICE_NAME }}..."
              
              TAILNET="${TAILSCALE_TAILNET}"
              if [[ -z "$TAILNET" ]]; then
                echo "⚠️ TAILSCALE_TAILNET not set, using default discovery"
                # For personal accounts, use the default endpoint without tailnet
                DEVICES_URL="https://api.tailscale.com/api/v2/devices"
              else
                echo "🔍 Using tailnet: $TAILNET"
                DEVICES_URL="https://api.tailscale.com/api/v2/tailnet/$TAILNET/devices"
              fi
              
              echo "🌐 Fetching ALL devices for comprehensive cleanup..."
              DEVICES_RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" -H "Authorization: Bearer $TAILSCALE_AUTH_KEY" "$DEVICES_URL" 2>/dev/null || echo "CURL_FAILED")
              
              # Extract HTTP status code
              HTTP_CODE=$(echo "$DEVICES_RESPONSE" | grep "HTTP_CODE:" | cut -d: -f2)
              DEVICES_JSON=$(echo "$DEVICES_RESPONSE" | sed '/HTTP_CODE:/d')
              
              echo "🔍 API Response Code: $HTTP_CODE"
              
              if [[ "$HTTP_CODE" == "200" ]] && echo "$DEVICES_JSON" | jq -e '.devices' >/dev/null 2>&1; then
                # Multiple cleanup patterns to catch all variants
                CLEANUP_PATTERNS=(
                  "^${{ env.SERVICE_NAME }}(-[0-9]+)?$"
                  "^${{ env.SERVICE_NAME }}$" 
                  "^${{ env.SERVICE_NAME }}-"
                  "${{ env.SERVICE_NAME }}"
                )
                
                TOTAL_REMOVED=0
                ALL_FOUND_DEVICES=""
                
                # First pass: collect all matching device IDs
                for pattern in "${CLEANUP_PATTERNS[@]}"; do
                  echo "🔍 Searching for devices matching pattern: $pattern"
                  
                  DEVICES=$(echo "$DEVICES_JSON" | jq -r --arg pattern "$pattern" '
                    .devices[]? | 
                    select(
                      (.hostname // .name | test($pattern; "i")) or 
                      (.name // .hostname | test($pattern; "i"))
                    ) | 
                    .id // empty' | grep -v '^$')
                  
                  if [[ -n "$DEVICES" ]]; then
                    ALL_FOUND_DEVICES="$ALL_FOUND_DEVICES $DEVICES"
                  fi
                done
                
                # Remove duplicates and clean up the list
                UNIQUE_DEVICES=$(echo "$ALL_FOUND_DEVICES" | tr ' ' '\n' | sort -u | grep -v '^$' || true)
                
                if [[ -n "$UNIQUE_DEVICES" ]]; then
                  echo "Found devices to remove:"
                  for device_id in $UNIQUE_DEVICES; do
                    if [[ -n "$device_id" ]]; then
                      # Get device details before removal
                      DEVICE_INFO=$(echo "$DEVICES_JSON" | jq -r --arg id "$device_id" '.devices[]? | select(.id == $id) | "\(.hostname // .name // "unknown") (\(.addresses[0] // "no-ip"))"')
                      echo "🗑️ Removing device: $device_id - $DEVICE_INFO"
                      
                      # Try both API endpoint formats
                      DELETE_URL="https://api.tailscale.com/api/v2/device/$device_id"
                      REMOVE_RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" -X DELETE \
                        -H "Authorization: Bearer $TAILSCALE_AUTH_KEY" \
                        "$DELETE_URL" 2>/dev/null || echo "CURL_FAILED")
                      
                      REMOVE_CODE=$(echo "$REMOVE_RESPONSE" | grep "HTTP_CODE:" | cut -d: -f2)
                      
                      if [[ "$REMOVE_CODE" == "200" || "$REMOVE_CODE" == "204" ]]; then
                        echo "   ✅ Successfully removed"
                        ((TOTAL_REMOVED++))
                      else
                        echo "   ⚠️ Primary endpoint failed (HTTP: $REMOVE_CODE), trying alternative..."
                        # Try alternative API endpoint format
                        if [[ -n "$TAILNET" ]]; then
                          ALT_DELETE_URL="https://api.tailscale.com/api/v2/tailnet/$TAILNET/devices/$device_id"
                        else
                          ALT_DELETE_URL="https://api.tailscale.com/api/v2/devices/$device_id"
                        fi
                        
                        ALT_REMOVE_RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" -X DELETE \
                          -H "Authorization: Bearer $TAILSCALE_AUTH_KEY" \
                          "$ALT_DELETE_URL" 2>/dev/null || echo "CURL_FAILED")
                        
                        ALT_REMOVE_CODE=$(echo "$ALT_REMOVE_RESPONSE" | grep "HTTP_CODE:" | cut -d: -f2)
                        if [[ "$ALT_REMOVE_CODE" == "200" || "$ALT_REMOVE_CODE" == "204" ]]; then
                          echo "   ✅ Successfully removed via alternative endpoint"
                          ((TOTAL_REMOVED++))
                        else
                          echo "   ❌ Failed to remove device $device_id (Primary: $REMOVE_CODE, Alt: $ALT_REMOVE_CODE)"
                        fi
                      fi
                      
                      # Rate limiting: small delay between deletions
                      sleep 1
                    fi
                  done
                  echo "✅ Comprehensive Tailscale cleanup completed - removed $TOTAL_REMOVED devices total"
                else
                  echo "ℹ️ No Tailscale devices found matching service name pattern: $SERVICE_PATTERN"
                fi
              elif [[ "$HTTP_CODE" == "401" ]]; then
                echo "❌ Tailscale API authentication failed - check TAILSCALE_AUTH_KEY"
              elif [[ "$HTTP_CODE" == "403" ]]; then
                echo "❌ Tailscale API access forbidden - check API key permissions"
              elif [[ "$HTTP_CODE" == "404" ]]; then
                echo "❌ Tailscale tailnet not found - check TAILSCALE_TAILNET value"
              else
                echo "⚠️ Could not retrieve Tailscale devices list"
                echo "HTTP Code: $HTTP_CODE"
                echo "Response: $DEVICES_JSON" | head -200
              fi
            else
              echo "⚠️ Tailscale API key not available"
            fi
          else
            echo "ℹ️ No existing server found for cleanup"
          fi

      - name: 💥 Destroy Existing Server
        env:
          LINODE_CLI_TOKEN: ${{ secrets.LINODE_CLI_TOKEN }}
        run: |
          echo "💥 Looking for existing ${{ env.SERVICE_NAME }} server to destroy..."
          
          # Find existing server
          EXISTING_SERVER=$(linode-cli linodes list --text --no-headers | grep "${{ env.SERVICE_NAME }}" | head -1)
          
          if [[ -n "$EXISTING_SERVER" ]]; then
            SERVER_ID=$(echo "$EXISTING_SERVER" | cut -f1)
            SERVER_LABEL=$(echo "$EXISTING_SERVER" | cut -f2)
            
            echo "🔥 Destroying server: $SERVER_LABEL (ID: $SERVER_ID)"
            linode-cli linodes delete "$SERVER_ID"
            
            echo "⏳ Waiting for server destruction to complete..."
            sleep 30
            
            echo "✅ Server destroyed successfully"
          else
            echo "ℹ️ No existing server found for ${{ env.SERVICE_NAME }}"
          fi

  # ============================================================================
  # Main Destroy Job (for destroy action)
  # ============================================================================
  destroy-service:
    name: 🗑️ Destroy Service
    runs-on: ubuntu-latest
    needs: preflight-checks
    if: needs.preflight-checks.outputs.should_destroy == 'true' && needs.preflight-checks.outputs.destroy_confirmed == 'true'
    
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4

      - name: 🔧 Setup Linode CLI
        if: inputs.destroy_scope == 'full-server'
        env:
          LINODE_CLI_TOKEN: ${{ secrets.LINODE_CLI_TOKEN }}
        run: |
          pip install linode-cli
          # Configure via environment variable to avoid config file issues
          export LINODE_CLI_TOKEN="${{ secrets.LINODE_CLI_TOKEN }}"
          # Test CLI access
          linode-cli --version

      - name: 🗑️ Execute Destruction
        env:
          LINODE_CLI_TOKEN: ${{ secrets.LINODE_CLI_TOKEN }}
          TAILSCALE_AUTH_KEY: ${{ secrets.TAILSCALE_AUTH_KEY }}
          TAILSCALE_TAILNET: ${{ secrets.TAILSCALE_TAILNET }}
        run: |
          echo "🗑️ Destroying ${{ env.SERVICE_NAME }}..."
          echo "Scope: ${{ inputs.destroy_scope }}"
          
          # Install dependencies for API calls
          sudo apt-get update && sudo apt-get install -y curl jq >/dev/null 2>&1 || true
          
          case "${{ inputs.destroy_scope }}" in
            "service-only")
              echo "🛑 Stopping service containers only..."
              # Logic for service-only destruction
              ;;
            "reset-service")
              echo "🧹 Resetting service to clean state..."
              # Logic for service reset
              ;;
            "full-server")
              echo "💥 Destroying entire server with cleanup..."
              
              # Find and cleanup server before destruction
              SERVER_INFO=$(linode-cli linodes list --text --no-headers | grep "${{ env.SERVICE_NAME }}" | head -1)
              if [[ -n "$SERVER_INFO" ]]; then
                SERVER_ID=$(echo "$SERVER_INFO" | cut -f1)
                SERVER_LABEL=$(echo "$SERVER_INFO" | cut -f2)
                
                # Try to get server IP for cleanup
                SERVER_DETAILS=$(linode-cli linodes view "$SERVER_ID" --text --no-headers)
                SERVER_IP_COL4=$(echo "$SERVER_DETAILS" | cut -f4)
                SERVER_IP_COL5=$(echo "$SERVER_DETAILS" | cut -f5)
                SERVER_IP_COL6=$(echo "$SERVER_DETAILS" | cut -f6)
                SERVER_IP_COL7=$(echo "$SERVER_DETAILS" | cut -f7)
                
                SERVER_IP=""
                for IP in "$SERVER_IP_COL4" "$SERVER_IP_COL5" "$SERVER_IP_COL6" "$SERVER_IP_COL7"; do
                  if [[ "$IP" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
                    SERVER_IP="$IP"
                    break
                  fi
                done
                
                if [[ -n "$SERVER_IP" ]]; then
                  echo "🧹 Cleaning up services on server before destruction..."
                  
                  # Generate SSH key for cleanup
                  ssh-keygen -t rsa -b 4096 -f ~/.ssh/cleanup_key -N "" -C "cleanup-${{ env.SERVICE_NAME }}" || true
                  
                  # Try to connect and cleanup
                  if timeout 30 ssh -i ~/.ssh/cleanup_key -o StrictHostKeyChecking=no -o ConnectTimeout=10 root@$SERVER_IP "echo 'Cleanup connection successful'" 2>/dev/null; then
                    echo "🔗 Connected for cleanup..."
                    
                    # Cleanup Tailscale
                    ssh -i ~/.ssh/cleanup_key -o StrictHostKeyChecking=no root@$SERVER_IP "tailscale logout || true" || true
                    
                    echo "✅ Service cleanup completed"
                  else
                    echo "⚠️ Could not connect for cleanup"
                  fi
                fi
                
                echo "Destroying: $SERVER_LABEL (ID: $SERVER_ID)"
                linode-cli linodes delete "$SERVER_ID"
                echo "✅ Server destroyed"
                
                # Cleanup Tailscale devices via API after server destruction (comprehensive)
                if [[ -n "$TAILSCALE_AUTH_KEY" ]]; then
                  echo "🔗 Final comprehensive Tailscale cleanup for service ${{ env.SERVICE_NAME }}..."
                  
                  TAILNET="${TAILSCALE_TAILNET}"
                  if [[ -z "$TAILNET" ]]; then
                    echo "⚠️ TAILSCALE_TAILNET not set, using default discovery"
                    DEVICES_URL="https://api.tailscale.com/api/v2/devices"
                  else
                    echo "🔍 Using tailnet: $TAILNET"
                    DEVICES_URL="https://api.tailscale.com/api/v2/tailnet/$TAILNET/devices"
                  fi
                  
                  echo "🌐 Fetching ALL remaining devices for final cleanup..."
                  DEVICES_RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" -H "Authorization: Bearer $TAILSCALE_AUTH_KEY" "$DEVICES_URL" 2>/dev/null || echo "CURL_FAILED")
                  
                  # Extract HTTP status code
                  HTTP_CODE=$(echo "$DEVICES_RESPONSE" | grep "HTTP_CODE:" | cut -d: -f2)
                  DEVICES_JSON=$(echo "$DEVICES_RESPONSE" | sed '/HTTP_CODE:/d')
                  
                  echo "🔍 API Response Code: $HTTP_CODE"
                  
                  if [[ "$HTTP_CODE" == "200" ]] && echo "$DEVICES_JSON" | jq -e '.devices' >/dev/null 2>&1; then
                    # Even more aggressive cleanup patterns for final cleanup
                    CLEANUP_PATTERNS=(
                      "^${{ env.SERVICE_NAME }}(-[0-9]+)?$"
                      "^${{ env.SERVICE_NAME }}$" 
                      "^${{ env.SERVICE_NAME }}-"
                      "${{ env.SERVICE_NAME }}"
                      ".*${{ env.SERVICE_NAME }}.*"  # Catch any device containing the service name
                    )
                    
                    TOTAL_REMOVED=0
                    ALL_FOUND_DEVICES=""
                    
                    # Collect all matching device IDs from all patterns
                    for pattern in "${CLEANUP_PATTERNS[@]}"; do
                      echo "🔍 Final scan for pattern: $pattern"
                      
                      DEVICES=$(echo "$DEVICES_JSON" | jq -r --arg pattern "$pattern" '
                        .devices[]? | 
                        select(
                          (.hostname // .name | test($pattern; "i")) or 
                          (.name // .hostname | test($pattern; "i"))
                        ) | 
                        .id // empty' | grep -v '^$')
                      
                      if [[ -n "$DEVICES" ]]; then
                        ALL_FOUND_DEVICES="$ALL_FOUND_DEVICES $DEVICES"
                      fi
                    done
                    
                    # Remove duplicates and clean up the list
                    UNIQUE_DEVICES=$(echo "$ALL_FOUND_DEVICES" | tr ' ' '\n' | sort -u | grep -v '^$' || true)
                    
                    if [[ -n "$UNIQUE_DEVICES" ]]; then
                      echo "Found orphaned devices for final removal:"
                      for device_id in $UNIQUE_DEVICES; do
                        if [[ -n "$device_id" ]]; then
                          # Get device details before removal
                          DEVICE_INFO=$(echo "$DEVICES_JSON" | jq -r --arg id "$device_id" '.devices[]? | select(.id == $id) | "\(.hostname // .name // "unknown") (\(.addresses[0] // "no-ip"))"')
                          echo "🗑️ Final cleanup - removing: $device_id - $DEVICE_INFO"
                          
                          # Try both API endpoint formats aggressively
                          DELETE_URL="https://api.tailscale.com/api/v2/device/$device_id"
                          REMOVE_RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" -X DELETE \
                            -H "Authorization: Bearer $TAILSCALE_AUTH_KEY" \
                            "$DELETE_URL" 2>/dev/null || echo "CURL_FAILED")
                          
                          REMOVE_CODE=$(echo "$REMOVE_RESPONSE" | grep "HTTP_CODE:" | cut -d: -f2)
                          
                          if [[ "$REMOVE_CODE" == "200" || "$REMOVE_CODE" == "204" ]]; then
                            echo "   ✅ Successfully removed"
                            ((TOTAL_REMOVED++))
                          else
                            echo "   ⚠️ Trying alternative endpoint (HTTP: $REMOVE_CODE)..."
                            # Try alternative API endpoint format
                            if [[ -n "$TAILNET" ]]; then
                              ALT_DELETE_URL="https://api.tailscale.com/api/v2/tailnet/$TAILNET/devices/$device_id"
                            else
                              ALT_DELETE_URL="https://api.tailscale.com/api/v2/devices/$device_id"
                            fi
                            
                            ALT_REMOVE_RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" -X DELETE \
                              -H "Authorization: Bearer $TAILSCALE_AUTH_KEY" \
                              "$ALT_DELETE_URL" 2>/dev/null || echo "CURL_FAILED")
                            
                            ALT_REMOVE_CODE=$(echo "$ALT_REMOVE_RESPONSE" | grep "HTTP_CODE:" | cut -d: -f2)
                            if [[ "$ALT_REMOVE_CODE" == "200" || "$ALT_REMOVE_CODE" == "204" ]]; then
                              echo "   ✅ Successfully removed via alternative endpoint"
                              ((TOTAL_REMOVED++))
                            else
                              echo "   ❌ Could not remove device $device_id (Primary: $REMOVE_CODE, Alt: $ALT_REMOVE_CODE)"
                            fi
                          fi
                          
                          # Rate limiting: small delay between deletions
                          sleep 1
                        fi
                      done
                      echo "✅ Final Tailscale cleanup completed - removed $TOTAL_REMOVED devices total"
                          echo "🗑️ Removing orphaned Tailscale device: $device_id - $DEVICE_INFO"
                          
                          DELETE_URL="https://api.tailscale.com/api/v2/device/$device_id"
                          REMOVE_RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" -X DELETE \
                            -H "Authorization: Bearer $TAILSCALE_AUTH_KEY" \
                            "$DELETE_URL" 2>/dev/null || echo "CURL_FAILED")
                          
                          REMOVE_CODE=$(echo "$REMOVE_RESPONSE" | grep "HTTP_CODE:" | cut -d: -f2)
                          
                          if [[ "$REMOVE_CODE" == "200" || "$REMOVE_CODE" == "204" ]]; then
                            echo "   ✅ Successfully removed"
                            ((REMOVED_COUNT++))
                          else
                            echo "   ❌ Failed to remove device $device_id (HTTP: $REMOVE_CODE)"
                          fi
                          
                          # Rate limiting
                          sleep 1
                        fi
                      done
                      echo "✅ Orphaned device cleanup completed - removed $REMOVED_COUNT devices"
                    else
                      echo "ℹ️ No orphaned Tailscale devices found matching pattern: $SERVICE_PATTERN"
                    fi
                  else
                    echo "⚠️ Could not retrieve Tailscale devices for cleanup - HTTP: $HTTP_CODE"
                  fi
                fi
              else
                echo "⚠️ No server found for ${{ env.SERVICE_NAME }}"
              fi
              ;;
          esac

  # ============================================================================
  # Server Infrastructure Setup
  # ============================================================================
  setup-infrastructure:
    name: 🏗️ Setup Infrastructure
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [preflight-checks, cleanup-old-resources, destroy-existing-server, build-docker-api, build-docker-web, build-docker-auth]
    if: |
      always() && 
      needs.preflight-checks.outputs.should_deploy == 'true' &&
      (needs.cleanup-old-resources.result == 'success' || needs.cleanup-old-resources.result == 'skipped') &&
      (needs.destroy-existing-server.result == 'success' || needs.destroy-existing-server.result == 'skipped') &&
      (needs.build-docker-api.result == 'success' || needs.build-docker-api.result == 'skipped') &&
      (needs.build-docker-web.result == 'success' || needs.build-docker-web.result == 'skipped') &&
      (needs.build-docker-auth.result == 'success' || needs.build-docker-auth.result == 'skipped')
    outputs:
      server_ip: ${{ steps.create-server.outputs.server_ip }}
      server_id: ${{ steps.create-server.outputs.server_id }}
      tailscale_ip: ${{ steps.stage2-setup.outputs.tailscale_ip }}
      ssh_private_key: ${{ steps.create-server.outputs.ssh_private_key }}
    
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4

      - name: 🔧 Setup Linode CLI
        env:
          LINODE_CLI_TOKEN: ${{ secrets.LINODE_CLI_TOKEN }}
        run: |
          pip install linode-cli
          # Configure via environment variable to avoid config file issues
          export LINODE_CLI_TOKEN="${{ secrets.LINODE_CLI_TOKEN }}"
          # Test CLI access
          linode-cli --version

      - name: 🚀 Create or Find Server
        id: create-server
        env:
          LINODE_CLI_TOKEN: ${{ secrets.LINODE_CLI_TOKEN }}
        run: |
          echo "🚀 Managing Linode server for ${{ env.SERVICE_NAME }}..."
          
          # Check if server already exists (unless we just destroyed it)
          if [[ "${{ inputs.overwrite_server }}" != "true" ]]; then
            EXISTING_SERVER=$(linode-cli linodes list --text --no-headers | grep "${{ env.SERVICE_NAME }}" | head -1)
            if [[ -n "$EXISTING_SERVER" ]]; then
              echo "🔍 Debug - Found existing server:"
              echo "$EXISTING_SERVER"
              
              SERVER_ID=$(echo "$EXISTING_SERVER" | cut -f1)
              # Try different columns for IP address
              SERVER_IP_COL4=$(echo "$EXISTING_SERVER" | cut -f4)
              SERVER_IP_COL5=$(echo "$EXISTING_SERVER" | cut -f5)
              SERVER_IP_COL6=$(echo "$EXISTING_SERVER" | cut -f6)
              SERVER_IP_COL7=$(echo "$EXISTING_SERVER" | cut -f7)
              
              echo "IP candidates: Col4='$SERVER_IP_COL4', Col5='$SERVER_IP_COL5', Col6='$SERVER_IP_COL6', Col7='$SERVER_IP_COL7'"
              
              # Use the first valid IP address we find
              for IP in "$SERVER_IP_COL4" "$SERVER_IP_COL5" "$SERVER_IP_COL6" "$SERVER_IP_COL7"; do
                if [[ "$IP" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
                  SERVER_IP="$IP"
                  break
                fi
              done
              
              if [[ -z "$SERVER_IP" ]]; then
                echo "❌ Could not extract IP address from server info"
                exit 1
              fi
              
              echo "✅ Using existing server: $SERVER_IP (ID: $SERVER_ID)"
              echo "server_ip=$SERVER_IP" >> $GITHUB_OUTPUT
              echo "server_id=$SERVER_ID" >> $GITHUB_OUTPUT
              exit 0
            fi
          fi
          
          # Create new server
          SERVER_LABEL="${{ env.SERVICE_NAME }}"
          echo "🆕 Creating new server: $SERVER_LABEL"
          
          # Generate SSH key for this deployment
          echo "🔑 Generating SSH key for server access..."
          ssh-keygen -t rsa -b 4096 -f ~/.ssh/linode_deployment_key -N "" -C "github-actions-${{ env.SERVICE_NAME }}"
          
          # Get the public key content for server authorization
          SSH_PUBLIC_KEY=$(cat ~/.ssh/linode_deployment_key.pub)
          echo "🔑 SSH Public Key: $SSH_PUBLIC_KEY"
          
          # Store the private key (base64 encoded for safe storage)
          SSH_PRIVATE_KEY=$(base64 -w 0 ~/.ssh/linode_deployment_key)
          echo "ssh_private_key=$SSH_PRIVATE_KEY" >> $GITHUB_OUTPUT
          
          echo "🚀 Creating server with SSH key authentication..."
          RESULT=$(linode-cli linodes create \
            --type "${{ env.SERVER_TYPE }}" \
            --region "${{ env.TARGET_REGION }}" \
            --image "linode/arch" \
            --label "$SERVER_LABEL" \
            --root_pass "${{ secrets.SERVICE_ROOT_PASSWORD }}" \
            --authorized_keys "$SSH_PUBLIC_KEY" \
            --backups_enabled=${{ inputs.enable_backups || 'false' }} \
            --text --no-headers)
          
          SERVER_ID=$(echo "$RESULT" | cut -f1)
          
          # Wait for server to be running
          echo "⏳ Waiting for server to be ready..."
          ATTEMPT=0
          while true; do
            # Get server info and check status
            SERVER_INFO=$(linode-cli linodes view "$SERVER_ID" --text --no-headers)
            
            # Debug: show the full output on first few attempts
            if [[ $ATTEMPT -lt 3 ]]; then
              echo "🔍 Debug - Server info columns:"
              echo "$SERVER_INFO"
            fi
            
            # Status is in column 6 (ID|Label|Region|Type|Image|Status|IP|Backups)
            STATUS=$(echo "$SERVER_INFO" | cut -f6)
            
            echo "Attempt $((++ATTEMPT)): Status='$STATUS'"
            
            # Check if server is running
            if [[ "$STATUS" == "running" ]]; then
              echo "✅ Server is running!"
              break
            fi
            
            # Don't wait forever for server status
            if [[ $ATTEMPT -gt 15 ]]; then
              echo "⚠️ Server status check timeout - proceeding to SSH test"
              break
            fi
            
            sleep 5  # Check more frequently
          done
          
          # Get server IP
          SERVER_INFO=$(linode-cli linodes view "$SERVER_ID" --text --no-headers)
          echo "🔍 Debug - Server view output:"
          echo "$SERVER_INFO"
          
          # Try different columns for IP address
          SERVER_IP_COL4=$(echo "$SERVER_INFO" | cut -f4)
          SERVER_IP_COL5=$(echo "$SERVER_INFO" | cut -f5)
          SERVER_IP_COL6=$(echo "$SERVER_INFO" | cut -f6)
          SERVER_IP_COL7=$(echo "$SERVER_INFO" | cut -f7)
          
          echo "IP candidates: Col4='$SERVER_IP_COL4', Col5='$SERVER_IP_COL5', Col6='$SERVER_IP_COL6', Col7='$SERVER_IP_COL7'"
          
          # Use the first valid IP address we find
          for IP in "$SERVER_IP_COL4" "$SERVER_IP_COL5" "$SERVER_IP_COL6" "$SERVER_IP_COL7"; do
            if [[ "$IP" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
              SERVER_IP="$IP"
              break
            fi
          done
          
          if [[ -z "$SERVER_IP" ]]; then
            echo "❌ Could not extract IP address from server info"
            exit 1
          fi
          
          echo "✅ Server ready: $SERVER_IP (ID: $SERVER_ID)"
          
          echo "server_ip=$SERVER_IP" >> $GITHUB_OUTPUT
          echo "server_id=$SERVER_ID" >> $GITHUB_OUTPUT

      - name: ⏳ Wait for SSH Access
        run: |
          echo "⏳ Waiting for SSH access to ${{ steps.create-server.outputs.server_ip }}..."
          
          SSH_READY=false
          
          # First, test basic connectivity
          echo "🔍 Testing basic connectivity to port 22..."
          for i in {1..10}; do
            if timeout 5 nc -zv ${{ steps.create-server.outputs.server_ip }} 22 2>/dev/null; then
              echo "✅ Port 22 is reachable on attempt $i"
              break
            fi
            echo "Port 22 not ready, waiting 10 seconds..."
            sleep 10
          done
          
          # Test SSH with detailed error output
          echo "🔑 Testing SSH connection with private key..."
          
          for i in {1..15}; do
            echo "Attempt $i/15: Testing SSH connection..."
            
            # Use the generated private key for authentication
            SSH_OUTPUT=$(timeout 10 ssh -i ~/.ssh/linode_deployment_key -v -o StrictHostKeyChecking=no -o ConnectTimeout=5 -o ConnectionAttempts=1 \
               root@${{ steps.create-server.outputs.server_ip }} "echo 'SSH ready'" 2>&1 || echo "SSH_FAILED")
            
            if echo "$SSH_OUTPUT" | grep -q "SSH ready"; then
              echo "✅ SSH ready after $i attempts"
              SSH_READY=true
              break
            else
              echo "SSH failed. Last few lines of output:"
              echo "$SSH_OUTPUT" | tail -3
            fi
            
            echo "Waiting 15 seconds before next attempt..."
            sleep 15
          done
          
          if [[ "$SSH_READY" != "true" ]]; then
            echo "❌ SSH failed to become ready after 30 attempts (5 minutes)"
            echo "🔍 Debugging SSH connection..."
            
            # Try to get more info about why SSH is failing
            echo "Testing basic connectivity..."
            timeout 5 nc -zv ${{ steps.create-server.outputs.server_ip }} 22 || echo "Port 22 not reachable"
            
            exit 1
          fi

      - name: 🏗️ Stage 1 - Pre-Reboot Setup
        id: stage1-setup
        run: |
          echo "🏗️ Stage 1: Pre-reboot foundation setup..."
          
          # Check if service has its own stage1 script
          if [ -f "scripts/stage1-setup.sh" ]; then
            echo "✅ Using service-specific stage1 script: scripts/stage1-setup.sh"
            cp scripts/stage1-setup.sh stage1-setup.sh
            
            # Inject environment variables into the script
            sed -i 's/\$ACTIONS_USER_PASSWORD/${{ secrets.ACTIONS_USER_PASSWORD }}/g' stage1-setup.sh
            sed -i 's/\$TAILSCALE_AUTH_KEY/${{ secrets.TAILSCALE_AUTH_KEY }}/g' stage1-setup.sh
          else
            echo "ℹ️ No service-specific stage1 script found, using generated script"
          
          cat > stage1-setup.sh << 'EOF'
          #!/bin/bash
          set -euo pipefail
          
          echo "🔄 Updating system..."
          # Update package databases first
          pacman -Sy --noconfirm
          
          echo "🔧 Optimizing package mirrors for better download speeds..."
          # Backup original mirrorlist
          cp /etc/pacman.d/mirrorlist /etc/pacman.d/mirrorlist.backup || true
          
          # Use faster mirrors and optimize pacman configuration
          {
            echo "## United States"
            echo "Server = https://america.mirror.pkgbuild.com/\$repo/os/\$arch"
            echo "Server = https://mirror.arizona.edu/archlinux/\$repo/os/\$arch"
            echo "Server = https://mirrors.ocf.berkeley.edu/archlinux/\$repo/os/\$arch"
            echo "Server = https://mirror.cs.pitt.edu/archlinux/\$repo/os/\$arch"
            echo "Server = https://mirrors.kernel.org/archlinux/\$repo/os/\$arch"
            echo ""
            echo "## Global CDN"
            echo "Server = https://geo.mirror.pkgbuild.com/\$repo/os/\$arch"
          } > /etc/pacman.d/mirrorlist
          
          # Optimize pacman for faster downloads
          sed -i 's/#ParallelDownloads = 5/ParallelDownloads = 10/' /etc/pacman.conf || true
          
          # Handle NVIDIA firmware conflicts with comprehensive approach
          echo "🔧 Removing conflicting NVIDIA firmware files..."
          rm -rf /usr/lib/firmware/nvidia/ad103* 2>/dev/null || true
          rm -rf /usr/lib/firmware/nvidia/ad104* 2>/dev/null || true  
          rm -rf /usr/lib/firmware/nvidia/ad106* 2>/dev/null || true
          rm -rf /usr/lib/firmware/nvidia/ad107* 2>/dev/null || true
          
          # Remove symlinks and directories that cause conflicts
          find /usr/lib/firmware/nvidia -type l -delete 2>/dev/null || true
          find /usr/lib/firmware/nvidia -name "*.zst" -delete 2>/dev/null || true
          
          # Force upgrade with multiple overwrite patterns and retry logic
          echo "🔄 Performing system upgrade with retry logic..."
          UPGRADE_SUCCESS=false
          for attempt in {1..3}; do
            echo "Upgrade attempt $attempt/3..."
            if pacman -Su --noconfirm --overwrite='*' --overwrite='/usr/lib/firmware/nvidia/*' 2>/dev/null; then
              UPGRADE_SUCCESS=true
              break
            else
              echo "⚠️ Upgrade attempt $attempt failed, trying alternative approach..."
              # Clean up and try again
              rm -rf /usr/lib/firmware/nvidia || true
              rm -f /var/lib/pacman/db.lck || true
              sleep 5
            fi
          done
          
          if [[ "$UPGRADE_SUCCESS" != "true" ]]; then
            echo "⚠️ System upgrade failed after 3 attempts, proceeding with package installation..."
          else
            echo "✅ System upgrade completed successfully"
          fi
          
          echo "📦 Installing core packages with retry logic..."
          # Install essential packages first (most important ones) - using modern Docker with Compose plugin
          CORE_PACKAGES="curl wget git docker docker-compose tailscale fail2ban sudo rsync"
          
          echo "🔄 Installing essential packages: $CORE_PACKAGES"
          INSTALL_SUCCESS=false
          for attempt in {1..3}; do
            echo "Package install attempt $attempt/3..."
            if timeout 300 pacman -S --noconfirm $CORE_PACKAGES; then
              INSTALL_SUCCESS=true
              echo "✅ Essential packages installed successfully"
              break
            else
              echo "⚠️ Package install attempt $attempt failed, retrying..."
              # Clear any locks and try again
              rm -f /var/lib/pacman/db.lck || true
              sleep 10
            fi
          done
          
          if [[ "$INSTALL_SUCCESS" != "true" ]]; then
            echo "❌ Failed to install essential packages after 3 attempts"
            echo "🔄 Trying to install packages individually..."
            
            # Try installing packages one by one
            for pkg in $CORE_PACKAGES; do
              echo "Installing $pkg..."
              timeout 120 pacman -S --noconfirm "$pkg" || echo "⚠️ Failed to install $pkg, continuing..."
            done
          fi
          
          echo "🐳 Ensuring modern Docker Compose is available..."
          # Install Docker Compose plugin for modern 'docker compose' command
          if ! docker compose version &>/dev/null; then
            echo "🔧 Installing Docker Compose plugin..."
            
            # Method 1: Try installing from AUR if available
            if command -v yay &>/dev/null || command -v paru &>/dev/null; then
              echo "📦 Installing docker-compose plugin via AUR helper..."
              if command -v yay &>/dev/null; then
                yay -S --noconfirm docker-compose-plugin || echo "⚠️ AUR installation failed"
              elif command -v paru &>/dev/null; then
                paru -S --noconfirm docker-compose-plugin || echo "⚠️ AUR installation failed"
              fi
            fi
            
            # Method 2: Manual installation if AUR failed
            if ! docker compose version &>/dev/null; then
              echo "🔧 Installing Docker Compose plugin manually..."
              
              # Create docker plugins directory
              mkdir -p /usr/local/lib/docker/cli-plugins
              
              # Download latest Docker Compose plugin
              COMPOSE_VERSION=$(curl -s https://api.github.com/repos/docker/compose/releases/latest | grep 'tag_name' | cut -d'"' -f4 | sed 's/v//')
              echo "📥 Downloading Docker Compose v$COMPOSE_VERSION..."
              
              curl -SL "https://github.com/docker/compose/releases/download/v$COMPOSE_VERSION/docker-compose-linux-x86_64" \
                -o /usr/local/lib/docker/cli-plugins/docker-compose
              
              chmod +x /usr/local/lib/docker/cli-plugins/docker-compose
              
              # Also install to /usr/local/bin for backward compatibility
              cp /usr/local/lib/docker/cli-plugins/docker-compose /usr/local/bin/docker-compose
              chmod +x /usr/local/bin/docker-compose
              
              # Create symlink for easy access
              ln -sf /usr/local/bin/docker-compose /usr/bin/docker-compose 2>/dev/null || true
              
              # Verify installation
              if /usr/local/lib/docker/cli-plugins/docker-compose version &>/dev/null; then
                echo "✅ Docker Compose plugin installed successfully"
                /usr/local/lib/docker/cli-plugins/docker-compose version
              else
                echo "⚠️ Docker Compose plugin installation may have issues"
              fi
              
              # Also verify standalone docker-compose command
              if docker-compose --version &>/dev/null; then
                echo "✅ Standalone docker-compose command also available"
                docker-compose --version
              fi
            fi
          else
            echo "✅ Docker Compose plugin already available"
            docker compose version
          fi
          
          echo "🌐 Setting up Docker networks with static IPs for services..."
          # Create dedicated networks for each service with predictable IP ranges
          # This allows proper iptables rules and Tailscale subnet sharing
          
          # Start Docker service temporarily to create networks
          systemctl start docker
          sleep 5
          
          # Remove any existing networks that might conflict
          docker network rm fks-network 2>/dev/null || true
          docker network rm ats-network 2>/dev/null || true  
          docker network rm nginx-network 2>/dev/null || true
          
          # Create service-specific networks with static IP ranges
          echo "🔧 Creating FKS network (172.20.0.0/16)..."
          docker network create \
            --driver bridge \
            --subnet=172.20.0.0/16 \
            --ip-range=172.20.1.0/24 \
            --gateway=172.20.0.1 \
            fks-network
          
          echo "🔧 Creating ATS network (172.21.0.0/16)..."
          docker network create \
            --driver bridge \
            --subnet=172.21.0.0/16 \
            --ip-range=172.21.1.0/24 \
            --gateway=172.21.0.1 \
            ats-network
          
          echo "🔧 Creating Nginx network (172.22.0.0/16)..."
          docker network create \
            --driver bridge \
            --subnet=172.22.0.0/16 \
            --ip-range=172.22.1.0/24 \
            --gateway=172.22.0.1 \
            nginx-network
          
          # Stop Docker service (will be started properly in Stage 2)
          systemctl stop docker
          
          echo "✅ Docker networks configured with static IP ranges"
          echo "   📍 FKS:   172.20.0.0/16 (containers: 172.20.1.0/24)"
          echo "   📍 ATS:   172.21.0.0/16 (containers: 172.21.1.0/24)" 
          echo "   📍 Nginx: 172.22.0.0/16 (containers: 172.22.1.0/24)"
          
          # Try to install base-devel (development tools) - optional
          echo "📦 Installing development tools (optional)..."
          timeout 240 pacman -S --noconfirm base-devel || {
            echo "⚠️ Failed to install base-devel, continuing without development tools..."
          }
          
          echo "👥 Creating users..."
          useradd -m -s /bin/bash jordan || true
          echo "jordan:${{ secrets.JORDAN_PASSWORD }}" | chpasswd
          usermod -aG wheel,docker jordan
          
          useradd -m -s /bin/bash actions_user || true
          echo "actions_user:${{ secrets.ACTIONS_USER_PASSWORD }}" | chpasswd
          usermod -aG wheel,docker actions_user
          
          useradd -m -s /bin/bash ${{ env.SERVICE_NAME }}_user || true
          usermod -aG docker ${{ env.SERVICE_NAME }}_user
          # Set a password for SSH access (temporary, will be secured via Tailscale)
          echo "${{ env.SERVICE_NAME }}_user:${{ secrets.ACTIONS_USER_PASSWORD }}" | chpasswd
          
          # Enable SSH access for service user
          mkdir -p /home/${{ env.SERVICE_NAME }}_user/.ssh
          chmod 700 /home/${{ env.SERVICE_NAME }}_user/.ssh
          chown ${{ env.SERVICE_NAME }}_user:${{ env.SERVICE_NAME }}_user /home/${{ env.SERVICE_NAME }}_user/.ssh
          
          # Enable password authentication for post-deployment access
          sed -i 's/#PasswordAuthentication yes/PasswordAuthentication yes/' /etc/ssh/sshd_config
          sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_config
          
          echo "🏷️ Setting hostname in Stage 1 (before Tailscale auth)..."
          # Set hostname to service name for better identification - MUST be done in Stage 1
          hostnamectl set-hostname "${{ env.SERVICE_NAME }}"
          echo "✅ Hostname set to: ${{ env.SERVICE_NAME }}"
          
          echo "⚙️ Enabling services for post-reboot..."
          systemctl enable docker
          systemctl enable tailscaled
          

          
          echo "� Creating post-reboot script..."
          cat > /usr/local/bin/stage2-post-reboot.sh << 'STAGE2EOF'
          #!/bin/bash
          set -euo pipefail
          
          echo "🚀 Stage 2: Post-reboot setup starting..."
          
          echo "🐳 Starting Docker..."
          
          echo "📦 Installing firewall packages after reboot..."
          # Install iptables and ufw after kernel update and reboot to avoid conflicts
          pacman -S --noconfirm ufw iptables-nft || {
            echo "🔄 Resolving iptables conflicts..."
            # Remove conflicting iptables package first
            pacman -Rdd --noconfirm iptables 2>/dev/null || true
            pacman -S --noconfirm iptables-nft ufw
          }
          
          echo "� Configuring firewall before starting services..."
          # Configure firewall first, before starting Docker
          ufw --force reset
          ufw default deny incoming
          ufw default allow outgoing
          ufw allow ssh
          
          # Initialize iptables chains that Docker expects
          echo "🔧 Initializing iptables chains for Docker..."
          iptables -t nat -N DOCKER 2>/dev/null || true
          iptables -t filter -N DOCKER 2>/dev/null || true
          iptables -t filter -N DOCKER-ISOLATION-STAGE-1 2>/dev/null || true
          iptables -t filter -N DOCKER-ISOLATION-STAGE-2 2>/dev/null || true
          iptables -t filter -N DOCKER-USER 2>/dev/null || true
          
          # Add basic Docker chains if they don't exist
          iptables -t nat -C PREROUTING -m addrtype --dst-type LOCAL -j DOCKER 2>/dev/null || \
            iptables -t nat -I PREROUTING -m addrtype --dst-type LOCAL -j DOCKER
          iptables -t nat -C OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER 2>/dev/null || \
            iptables -t nat -I OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER
          iptables -t filter -C FORWARD -j DOCKER-USER 2>/dev/null || \
            iptables -t filter -I FORWARD -j DOCKER-USER
          iptables -t filter -C FORWARD -j DOCKER-ISOLATION-STAGE-1 2>/dev/null || \
            iptables -t filter -I FORWARD -j DOCKER-ISOLATION-STAGE-1
          iptables -t filter -C DOCKER-USER -j RETURN 2>/dev/null || \
            iptables -t filter -A DOCKER-USER -j RETURN
          
          echo "🐳 Starting Docker service..."
          systemctl start docker
          
          # Wait for Docker to be ready
          echo "⏳ Waiting for Docker to be ready..."
          for i in {1..10}; do
            if docker info >/dev/null 2>&1; then
              echo "✅ Docker is ready"
              break
            fi
            echo "Attempt $i/10: Waiting for Docker..."
            sleep 5
          done
          
          echo "🔧 Ensuring Docker Compose is available after reboot..."
          # Verify Docker Compose is still available after reboot
          if ! docker compose version &>/dev/null && ! docker-compose --version &>/dev/null; then
            echo "⚠️ Docker Compose not found after reboot, reinstalling..."
            
            # Recreate symlinks and paths
            if [[ -f "/usr/local/lib/docker/cli-plugins/docker-compose" ]]; then
              chmod +x /usr/local/lib/docker/cli-plugins/docker-compose
              cp /usr/local/lib/docker/cli-plugins/docker-compose /usr/local/bin/docker-compose
              chmod +x /usr/local/bin/docker-compose
              ln -sf /usr/local/bin/docker-compose /usr/bin/docker-compose 2>/dev/null || true
            else
              echo "🔧 Reinstalling Docker Compose..."
              mkdir -p /usr/local/lib/docker/cli-plugins
              COMPOSE_VERSION=$(curl -s https://api.github.com/repos/docker/compose/releases/latest | grep 'tag_name' | cut -d'"' -f4 | sed 's/v//')
              curl -SL "https://github.com/docker/compose/releases/download/v$COMPOSE_VERSION/docker-compose-linux-x86_64" \
                -o /usr/local/lib/docker/cli-plugins/docker-compose
              chmod +x /usr/local/lib/docker/cli-plugins/docker-compose
              cp /usr/local/lib/docker/cli-plugins/docker-compose /usr/local/bin/docker-compose
              chmod +x /usr/local/bin/docker-compose
              ln -sf /usr/local/bin/docker-compose /usr/bin/docker-compose 2>/dev/null || true
            fi
          fi
          
          # Verify Docker Compose is working
          echo "🔍 Verifying Docker Compose installation..."
          if docker compose version &>/dev/null; then
            echo "✅ 'docker compose' command working"
            docker compose version
          elif docker-compose --version &>/dev/null; then
            echo "✅ 'docker-compose' command working"
            docker-compose --version
          else
            echo "❌ Docker Compose installation failed"
          fi
          
          echo "🌐 Recreating Docker networks with static IPs..."
          # Recreate the service networks (they may not persist through reboot)
          docker network rm fks-network 2>/dev/null || true
          docker network rm ats-network 2>/dev/null || true  
          docker network rm nginx-network 2>/dev/null || true
          
          # Create service-specific networks with static IP ranges
          echo "🔧 Creating FKS network (172.20.0.0/16)..."
          docker network create \
            --driver bridge \
            --subnet=172.20.0.0/16 \
            --ip-range=172.20.1.0/24 \
            --gateway=172.20.0.1 \
            fks-network
          
          echo "🔧 Creating ATS network (172.21.0.0/16)..."
          docker network create \
            --driver bridge \
            --subnet=172.21.0.0/16 \
            --ip-range=172.21.1.0/24 \
            --gateway=172.21.0.1 \
            ats-network
          
          echo "🔧 Creating Nginx network (172.22.0.0/16)..."
          docker network create \
            --driver bridge \
            --subnet=172.22.0.0/16 \
            --ip-range=172.22.1.0/24 \
            --gateway=172.22.0.1 \
            nginx-network
          
          echo "🔧 Configuring iptables rules for Docker networks..."
          # Allow traffic between service networks for inter-service communication
          iptables -I DOCKER-USER -s 172.20.0.0/16 -d 172.21.0.0/16 -j ACCEPT 2>/dev/null || true  # FKS → ATS
          iptables -I DOCKER-USER -s 172.20.0.0/16 -d 172.22.0.0/16 -j ACCEPT 2>/dev/null || true  # FKS → Nginx
          iptables -I DOCKER-USER -s 172.21.0.0/16 -d 172.20.0.0/16 -j ACCEPT 2>/dev/null || true  # ATS → FKS
          iptables -I DOCKER-USER -s 172.21.0.0/16 -d 172.22.0.0/16 -j ACCEPT 2>/dev/null || true  # ATS → Nginx
          iptables -I DOCKER-USER -s 172.22.0.0/16 -d 172.20.0.0/16 -j ACCEPT 2>/dev/null || true  # Nginx → FKS
          iptables -I DOCKER-USER -s 172.22.0.0/16 -d 172.21.0.0/16 -j ACCEPT 2>/dev/null || true  # Nginx → ATS
          
          echo "✅ Docker networks configured with static IP ranges"
          echo "   📍 FKS:   172.20.0.0/16 (containers: 172.20.1.0/24)"
          echo "   📍 ATS:   172.21.0.0/16 (containers: 172.21.1.0/24)" 
          echo "   📍 Nginx: 172.22.0.0/16 (containers: 172.22.1.0/24)"
          
          echo "📝 Creating network configuration file for services..."
          # Create a configuration file that services can use for network setup
          cat > /opt/docker-networks.conf << 'NETWORKS_EOF'
          # Docker Network Configuration for Services
          # This file is sourced by service start scripts to use correct networks
          
          # Network Names and Subnets
          FKS_NETWORK_NAME="fks-network"
          FKS_NETWORK_SUBNET="172.20.0.0/16"
          FKS_NETWORK_GATEWAY="172.20.0.1"
          FKS_CONTAINER_RANGE="172.20.1.0/24"
          
          ATS_NETWORK_NAME="ats-network"
          ATS_NETWORK_SUBNET="172.21.0.0/16"
          ATS_NETWORK_GATEWAY="172.21.0.1"
          ATS_CONTAINER_RANGE="172.21.1.0/24"
          
          NGINX_NETWORK_NAME="nginx-network"
          NGINX_NETWORK_SUBNET="172.22.0.0/16"
          NGINX_NETWORK_GATEWAY="172.22.0.1"
          NGINX_CONTAINER_RANGE="172.22.1.0/24"
          
          # Service-specific IP assignments (for static IPs if needed)
          FKS_API_IP="172.20.1.10"
          FKS_WEB_IP="172.20.1.11"
          FKS_AUTH_IP="172.20.1.12"
          
          ATS_GAME_IP="172.21.1.10"
          ATS_WEB_IP="172.21.1.11"
          
          NGINX_PROXY_IP="172.22.1.10"
          NGINX_WEB_IP="172.22.1.11"
          
          # All networks (for Tailscale advertisement)
          ALL_DOCKER_SUBNETS="172.17.0.0/16,172.20.0.0/16,172.21.0.0/16,172.22.0.0/16"
          NETWORKS_EOF
          
          chmod 644 /opt/docker-networks.conf
          echo "✅ Network configuration saved to /opt/docker-networks.conf"
          
          echo "🔗 Starting and authenticating Tailscale..."
          systemctl start tailscaled
          
          # Wait for tailscaled to be ready
          echo "⏳ Waiting for tailscaled daemon to start..."
          for i in {1..10}; do
            if systemctl is-active tailscaled >/dev/null 2>&1; then
              echo "✅ Tailscaled daemon is active"
              break
            fi
            echo "Attempt $i/10: Waiting for tailscaled..."
            sleep 3
          done
          
          # Validate auth key format before attempting connection
          AUTH_KEY="${{ secrets.TAILSCALE_AUTH_KEY }}"
          if [[ -z "$AUTH_KEY" ]]; then
            echo "❌ TAILSCALE_AUTH_KEY is empty"
            exit 1
          fi
          
          if [[ ! "$AUTH_KEY" =~ ^tskey- ]]; then
            echo "⚠️ Warning: Auth key doesn't start with 'tskey-' - this may not be a valid key"
          fi
          
          echo "🔗 Authenticating with Tailscale and advertising Docker subnets..."
          # Multiple authentication attempts with different strategies
          TAILSCALE_CONNECTED=false
          
          # Define all Docker subnets to advertise
          DOCKER_SUBNETS="172.17.0.0/16,172.20.0.0/16,172.21.0.0/16,172.22.0.0/16"
          echo "📍 Will advertise Docker subnets: $DOCKER_SUBNETS"
          
          # Attempt 1: Standard connection with subnet advertisement
          echo "🔄 Attempt 1: Standard Tailscale connection with Docker subnets..."
          if tailscale up --authkey="$AUTH_KEY" --hostname="${{ env.SERVICE_NAME }}" --accept-routes --advertise-routes="$DOCKER_SUBNETS" --timeout=180s; then
            TAILSCALE_CONNECTED=true
            echo "✅ Tailscale connected on first attempt with Docker subnets"
          else
            echo "⚠️ First attempt failed, trying alternative methods..."
            
            # Attempt 2: Reset and reconnect with tags
            echo "🔄 Attempt 2: Reset and reconnect with server tags..."
            tailscale logout 2>/dev/null || true
            sleep 5
            
            if tailscale up --authkey="$AUTH_KEY" --hostname="${{ env.SERVICE_NAME }}" --accept-routes --advertise-routes="$DOCKER_SUBNETS" --advertise-tags=tag:server --timeout=180s; then
              TAILSCALE_CONNECTED=true
              echo "✅ Tailscale connected on second attempt with Docker subnets"
            else
              echo "⚠️ Second attempt failed, trying without hostname..."
              
              # Attempt 3: Without custom hostname but with subnets
              echo "🔄 Attempt 3: Connection without custom hostname but with Docker subnets..."
              if tailscale up --authkey="$AUTH_KEY" --accept-routes --advertise-routes="$DOCKER_SUBNETS" --timeout=180s; then
                TAILSCALE_CONNECTED=true
                echo "✅ Tailscale connected without custom hostname but with Docker subnets"
              else
                echo "⚠️ Third attempt failed, trying basic connection without subnets..."
                
                # Attempt 4: Basic connection without subnets as last resort
                echo "🔄 Attempt 4: Basic connection without Docker subnets..."
                if tailscale up --authkey="$AUTH_KEY" --accept-routes --timeout=180s; then
                  TAILSCALE_CONNECTED=true
                  echo "✅ Tailscale connected with basic configuration (subnets can be added later)"
                else
                  echo "❌ All Tailscale connection attempts failed"
                  echo "🔍 Tailscale status:"
                  tailscale status 2>&1 || echo "Status command failed"
                  echo "🔍 Tailscaled logs:"
                  journalctl -u tailscaled --no-pager -l --since='5 minutes ago' | tail -20 || echo "No logs available"
                fi
              fi
            fi
          fi
          
          # Wait for Tailscale to be ready and get IP
          if [[ "$TAILSCALE_CONNECTED" == "true" ]]; then
            echo "⏳ Waiting for Tailscale network to be ready..."
            TAILSCALE_IP="pending"
            
            for i in {1..30}; do
              if tailscale status | grep -q "Logged in"; then
                # Try to get IP address
                CURRENT_IP=$(tailscale ip -4 2>/dev/null || echo "")
                if [[ -n "$CURRENT_IP" && "$CURRENT_IP" != "" ]]; then
                  TAILSCALE_IP="$CURRENT_IP"
                  echo "✅ Tailscale fully connected - IP: $TAILSCALE_IP"
                  break
                fi
              fi
              echo "Attempt $i/30: Waiting for Tailscale IP assignment..."
              sleep 10
            done
            
            echo "$TAILSCALE_IP" > /tmp/tailscale_ip
            
            if [[ "$TAILSCALE_IP" == "pending" ]]; then
              echo "⚠️ Tailscale connected but IP not assigned yet - this may resolve shortly"
            fi
          else
            echo "❌ Tailscale connection failed - proceeding without VPN"
            echo "pending" > /tmp/tailscale_ip
          fi
          
          echo "🔥 Completing firewall configuration with Tailscale rules..."
          # Add Tailscale-specific rules now that Tailscale is connected
          ufw allow in on tailscale0
          ufw --force enable
          
          echo "🔐 Configuring SSH for service access..."
          # Allow password authentication for Tailscale network only
          sed -i 's/#PasswordAuthentication yes/PasswordAuthentication yes/' /etc/ssh/sshd_config
          sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_config
          # Restart SSH service to apply changes
          systemctl restart sshd
          
          # Verify service user can be accessed
          echo "🔍 Verifying service user configuration..."
          id ${{ env.SERVICE_NAME }}_user || echo "⚠️ Service user not found"
          ls -la /home/${{ env.SERVICE_NAME }}_user/.ssh 2>/dev/null || echo "ℹ️ SSH directory not found for service user"
          
          echo "📊 Starting monitoring services..."
          echo "✅ Stage 2 complete - server ready for service deployment"
          STAGE2EOF
          
          chmod +x /usr/local/bin/stage2-post-reboot.sh
          
          echo "🔄 Creating systemd service for post-reboot setup..."
          cat > /etc/systemd/system/stage2-setup.service << 'SERVICEEOF'
          [Unit]
          Description=Stage 2 Post-Reboot Setup
          After=network-online.target
          Wants=network-online.target
          
          [Service]
          Type=oneshot
          ExecStart=/usr/local/bin/stage2-post-reboot.sh
          RemainAfterExit=yes
          StandardOutput=journal
          StandardError=journal
          
          [Install]
          WantedBy=multi-user.target
          SERVICEEOF
          
          systemctl enable stage2-setup.service
          
          echo "✅ Stage 1 complete - system ready for reboot"
          echo "NEEDS_REBOOT" > /tmp/stage1_status
          EOF
          fi
          
          scp -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no stage1-setup.sh root@${{ steps.create-server.outputs.server_ip }}:/tmp/
          ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} "chmod +x /tmp/stage1-setup.sh && /tmp/stage1-setup.sh"
          
          STAGE1_STATUS=$(ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} "cat /tmp/stage1_status" || echo "unknown")
          echo "stage1_status=$STAGE1_STATUS" >> $GITHUB_OUTPUT

      - name: 🔄 Reboot Server for Kernel Updates
        run: |
          echo "🔄 Rebooting server for kernel updates and service initialization..."
          ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} "reboot" || true
          
          echo "⏳ Waiting for server to come back online..."
          sleep 45  # Give more time for reboot
          
          # Wait for SSH to be available again
          for i in {1..20}; do
            if ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no -o ConnectTimeout=10 root@${{ steps.create-server.outputs.server_ip }} "echo 'SSH ready after reboot'"; then
              echo "✅ Server is back online after reboot"
              break
            fi
            echo "Attempt $i/20: Waiting for server to come back online..."
            sleep 15
          done

      - name: 🏗️ Stage 2 - Post-Reboot Verification
        id: stage2-setup
        run: |
          echo "🏗️ Stage 2: Verifying post-reboot setup..."
          
          # Wait for SSH to be available after reboot (servers take time to reboot)
          echo "⏳ Waiting for server to come back online after reboot..."
          SSH_READY=false
          
          for i in {1..20}; do
            echo "Attempt $i/20: Testing SSH connection after reboot..."
            
            if timeout 10 ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no -o ConnectTimeout=5 \
               root@${{ steps.create-server.outputs.server_ip }} "echo 'SSH ready after reboot'" 2>/dev/null; then
              echo "✅ SSH ready after reboot (attempt $i)"
              SSH_READY=true
              break
            fi
            sleep 15  # Wait longer between attempts for reboot
          done
          
          if [[ "$SSH_READY" != "true" ]]; then
            echo "❌ SSH failed to become ready after reboot"
            exit 1
          fi
          
          # Wait for Stage 2 systemd service to complete
          echo "⏳ Waiting for stage2-setup.service to complete..."
          for i in {1..12}; do  # Wait up to 3 minutes (12 * 15s)
            SERVICE_STATUS=$(ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} \
              "systemctl is-active stage2-setup.service 2>/dev/null || echo 'inactive'")
            
            echo "Attempt $i/12: Stage 2 service status: $SERVICE_STATUS"
            
            # For oneshot services: inactive means it completed (successfully or failed)
            if [[ "$SERVICE_STATUS" == "inactive" ]]; then
              # Check if it completed successfully
              EXIT_STATUS=$(ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} \
                "systemctl show stage2-setup.service --property=ExecMainStatus --value 2>/dev/null || echo 'unknown'")
              
              if [[ "$EXIT_STATUS" == "0" ]]; then
                echo "✅ Stage 2 service completed successfully"
                break
              else
                echo "⚠️ Stage 2 service completed with exit status: $EXIT_STATUS"
                # Show logs for debugging
                ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} \
                  "journalctl -u stage2-setup.service --no-pager -l --since='5 minutes ago'" || true
                break
              fi
            fi
            
            sleep 15
          done
          
          # Get Tailscale IP (multiple methods)
          TAILSCALE_IP=$(ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} \
            "tailscale ip -4 2>/dev/null || cat /tmp/tailscale_ip 2>/dev/null || echo 'pending'")
          echo "🔗 Tailscale IP: $TAILSCALE_IP"
          echo "tailscale_ip=$TAILSCALE_IP" >> $GITHUB_OUTPUT
          
          # Verify essential services
          echo "🔍 Verifying essential services..."
          ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@${{ steps.create-server.outputs.server_ip }} \
            "systemctl is-active docker && echo '✅ Docker is active'" || echo "⚠️ Docker may not be active"
          
          echo "✅ Stage 2 verification complete - server ready for service deployment"

  # ============================================================================
  # Service Deployment
  # ============================================================================
  deploy-service:
    name: 🚢 Deploy Service
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: [preflight-checks, setup-infrastructure]
    if: needs.preflight-checks.outputs.should_deploy == 'true'
    
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4

      - name: 🚢 Deploy Service
        run: |
          echo "🚢 Deploying ${{ env.SERVICE_NAME }}..."
          
          SERVER_IP="${{ needs.setup-infrastructure.outputs.server_ip }}"
          
          # Ensure SSH directory exists
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          
          # Try to create the SSH private key file if the key is available
          if [[ -n "${{ needs.setup-infrastructure.outputs.ssh_private_key }}" ]]; then
            echo "🔑 Setting up SSH key authentication..."
            echo "${{ needs.setup-infrastructure.outputs.ssh_private_key }}" | base64 -d > ~/.ssh/linode_deployment_key
            chmod 600 ~/.ssh/linode_deployment_key
            SSH_KEY_AVAILABLE=true
          else
            echo "⚠️ SSH key not available, will use password authentication"
            SSH_KEY_AVAILABLE=false
          fi
          
          # Install sshpass for password authentication fallback
          sudo apt-get update && sudo apt-get install -y sshpass >/dev/null 2>&1 || true
          
          # Create deployment script
          cat > deploy-script.sh << 'EOF'
          #!/bin/bash
          set -euo pipefail
          
          echo "🚢 Deploying ${{ env.SERVICE_NAME }} service..."
          
          # Setup service directory
          SERVICE_USER="${{ env.SERVICE_NAME }}_user"
          SERVICE_DIR="/home/$SERVICE_USER/${{ env.SERVICE_NAME }}"
          
          echo "📁 Setting up service directory..."
          
          # Clone or update repository
          if [[ -d "$SERVICE_DIR" ]]; then
            echo "📂 Repository exists, updating..."
            cd "$SERVICE_DIR"
            sudo -u $SERVICE_USER git pull origin main || sudo -u $SERVICE_USER git pull origin master || echo "Git pull failed, continuing..."
          else
            echo "📥 Cloning repository..."
            sudo -u $SERVICE_USER git clone "https://github.com/nuniesmith/${{ env.SERVICE_NAME }}.git" "$SERVICE_DIR"
            cd "$SERVICE_DIR"
          fi
          
          # Ensure proper ownership
          chown -R $SERVICE_USER:$SERVICE_USER "$SERVICE_DIR"
          
          echo "� Setting up environment..."
          
          # Copy any environment files if they exist
          if [[ -f ".env.example" ]]; then
            echo "📋 Setting up environment from example..."
            sudo -u $SERVICE_USER cp .env.example .env 2>/dev/null || true
          fi
          
          # Ensure service user can access Docker
          usermod -aG docker $SERVICE_USER 2>/dev/null || true
          
          # Test Docker access for service user
          echo "🐳 Testing Docker access for service user..."
          if sudo -u $SERVICE_USER docker info >/dev/null 2>&1; then
            echo "✅ Service user has Docker access"
          else
            echo "⚠️ Service user may not have Docker access, fixing..."
            # Force add user to docker group and restart Docker
            usermod -aG docker $SERVICE_USER
            systemctl restart docker
            sleep 5
          fi
          
          echo "🚀 Starting service using start.sh script..."
          
          # Check if start.sh exists and is executable
          if [[ -f "start.sh" ]]; then
            chmod +x start.sh
            echo "📋 Found start.sh script, executing as service user..."
            
            # Execute start.sh as the service user
            sudo -u $SERVICE_USER ./start.sh
            
            echo "✅ Service deployment completed"
          else
            echo "⚠️ No start.sh script found, attempting Docker Compose deployment..."
            
            # Fallback to docker compose if start.sh doesn't exist
            if [[ -f "docker-compose.yml" ]]; then
              echo "📋 Found docker-compose.yml, starting services..."
              sudo -u $SERVICE_USER docker compose up -d
              echo "✅ Docker Compose deployment completed"
            else
              echo "❌ Neither start.sh nor docker-compose.yml found!"
              exit 1
            fi
          fi
          
          # Wait for services to be ready
          echo "⏳ Waiting for services to start..."
          sleep 10
          
          # Basic health check
          echo "🏥 Performing basic health check..."
          if docker ps --format "table {{.Names}}\t{{.Status}}" | grep -v "NAMES"; then
            echo "✅ Services are running"
          else
            echo "⚠️ No containers found running"
          fi
          
          echo "✅ Service deployment completed successfully"
          EOF
          
          # Transfer and execute deployment script using the best available method
          if [[ "$SSH_KEY_AVAILABLE" == "true" ]]; then
            echo "🔑 Using SSH key authentication..."
            scp -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no deploy-script.sh root@$SERVER_IP:/tmp/
            ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@$SERVER_IP "chmod +x /tmp/deploy-script.sh && /tmp/deploy-script.sh"
          else
            echo "🔑 Using password authentication with actions_user..."
            # Connect as actions_user which was set up in Stage 1
            export SSHPASS="${{ secrets.ACTIONS_USER_PASSWORD }}"
            sshpass -e scp -o StrictHostKeyChecking=no deploy-script.sh actions_user@$SERVER_IP:/tmp/
            sshpass -e ssh -o StrictHostKeyChecking=no actions_user@$SERVER_IP "sudo chmod +x /tmp/deploy-script.sh && sudo /tmp/deploy-script.sh"
          fi

      - name: 🌐 Update DNS Records with Service IPs
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ZONE_ID: ${{ secrets.CLOUDFLARE_ZONE_ID }}
          TAILSCALE_IP: ${{ needs.setup-infrastructure.outputs.tailscale_ip }}
          PUBLIC_IP: ${{ needs.setup-infrastructure.outputs.server_ip }}
          CUSTOM_DOMAINS: ${{ inputs.custom_domains }}
        if: env.CLOUDFLARE_API_TOKEN != '' && env.CLOUDFLARE_ZONE_ID != ''
        run: |
          echo "🌐 Updating DNS records for ${{ env.SERVICE_NAME }}..."
          
          # Install dependencies
          sudo apt-get update && sudo apt-get install -y curl jq >/dev/null 2>&1 || true
          
          # Determine which IP to use (prefer Tailscale, fallback to public)
          if [[ "$TAILSCALE_IP" != "pending" && -n "$TAILSCALE_IP" ]]; then
            UPDATE_IP="$TAILSCALE_IP"
            echo "🔗 Using Tailscale IP: $UPDATE_IP"
          else
            UPDATE_IP="$PUBLIC_IP"
            echo "🌐 Using public IP: $UPDATE_IP"
          fi
          
          # Dynamic domain configuration based on service
          declare -a DOMAINS
          if [[ -n "$CUSTOM_DOMAINS" ]]; then
            # Use custom domains if provided
            IFS=',' read -ra DOMAINS <<< "$CUSTOM_DOMAINS"
            echo "� Using custom domains: ${DOMAINS[*]}"
          else
            # Auto-detect domains based on service name
            case "${{ env.SERVICE_NAME }}" in
              "nginx")
                DOMAINS=("7gram.xyz" "www.7gram.xyz" "fkstrading.xyz" "admin.7gram.xyz" "monitor.7gram.xyz" "nodes.7gram.xyz")
                ;;
              "fks-auth")
                DOMAINS=("auth.7gram.xyz" "identity.7gram.xyz")
                ;;
              "fks-api")
                DOMAINS=("api.7gram.xyz" "data.7gram.xyz" "worker.7gram.xyz")
                ;;
              "fks-web")
                DOMAINS=("fkstrading.xyz" "www.7gram.xyz")
                ;;
              "ats")
                DOMAINS=("ats.7gram.xyz" "game.7gram.xyz")
                ;;
              *)
                DOMAINS=("${{ env.SERVICE_NAME }}.${{ inputs.domain_suffix }}")
                ;;
            esac
            echo "🏷️ Auto-detected domains for ${{ env.SERVICE_NAME }}: ${DOMAINS[*]}"
          fi
          
          # Test Cloudflare API connectivity
          echo "🔧 Testing Cloudflare API connectivity..."
          API_TEST_RESPONSE=$(curl -s -X GET "https://api.cloudflare.com/client/v4/zones/$CLOUDFLARE_ZONE_ID" \
            -H "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
            -H "Content-Type: application/json")
          
          API_SUCCESS=$(echo "$API_TEST_RESPONSE" | jq -r '.success')
          if [[ "$API_SUCCESS" != "true" ]]; then
            echo "❌ Cloudflare API test failed:"
            echo "$API_TEST_RESPONSE" | jq -r '.errors[]?.message // "Unknown error"'
            exit 1
          fi
          
          ZONE_NAME=$(echo "$API_TEST_RESPONSE" | jq -r '.result.name')
          echo "✅ API connection successful - Zone: $ZONE_NAME"
          
          # Update DNS records for all domains
          UPDATED_COUNT=0
          FAILED_COUNT=0
          
          for DOMAIN_NAME in "${DOMAINS[@]}"; do
            echo ""
            echo "� Processing domain: $DOMAIN_NAME"
            
            # Check for existing DNS record
            EXISTING_RESPONSE=$(curl -s -X GET \
              "https://api.cloudflare.com/client/v4/zones/$CLOUDFLARE_ZONE_ID/dns_records?name=$DOMAIN_NAME&type=A" \
              -H "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
              -H "Content-Type: application/json")
            
            RECORD_ID=$(echo "$EXISTING_RESPONSE" | jq -r '.result[0].id // empty')
            CURRENT_IP=$(echo "$EXISTING_RESPONSE" | jq -r '.result[0].content // empty')
            
            if [[ "$CURRENT_IP" == "$UPDATE_IP" ]]; then
              echo "✅ $DOMAIN_NAME already points to $UPDATE_IP (no update needed)"
              ((UPDATED_COUNT++))
              continue
            fi
            
            # Update or create DNS record
            if [[ -n "$RECORD_ID" && "$RECORD_ID" != "null" ]]; then
              # Update existing record
              echo "📝 Updating existing DNS record: $DOMAIN_NAME ($CURRENT_IP → $UPDATE_IP)"
              UPDATE_RESPONSE=$(curl -s -X PUT \
                "https://api.cloudflare.com/client/v4/zones/$CLOUDFLARE_ZONE_ID/dns_records/$RECORD_ID" \
                -H "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
                -H "Content-Type: application/json" \
                --data "{
                  \"type\": \"A\",
                  \"name\": \"$DOMAIN_NAME\",
                  \"content\": \"$UPDATE_IP\",
                  \"ttl\": 120,
                  \"proxied\": false,
                  \"comment\": \"Updated by GitHub Actions for ${{ env.SERVICE_NAME }}\"
                }")
            else
              # Create new record
              echo "➕ Creating new DNS record: $DOMAIN_NAME → $UPDATE_IP"
              UPDATE_RESPONSE=$(curl -s -X POST \
                "https://api.cloudflare.com/client/v4/zones/$CLOUDFLARE_ZONE_ID/dns_records" \
                -H "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
                -H "Content-Type: application/json" \
                --data "{
                  \"type\": \"A\",
                  \"name\": \"$DOMAIN_NAME\",
                  \"content\": \"$UPDATE_IP\",
                  \"ttl\": 120,
                  \"proxied\": false,
                  \"comment\": \"Created by GitHub Actions for ${{ env.SERVICE_NAME }}\"
                }")
            fi
            
            # Check result
            UPDATE_SUCCESS=$(echo "$UPDATE_RESPONSE" | jq -r '.success')
            if [[ "$UPDATE_SUCCESS" == "true" ]]; then
              NEW_RECORD_ID=$(echo "$UPDATE_RESPONSE" | jq -r '.result.id')
              echo "✅ $DOMAIN_NAME updated successfully (Record ID: $NEW_RECORD_ID)"
              ((UPDATED_COUNT++))
            else
              echo "❌ Failed to update $DOMAIN_NAME:"
              echo "$UPDATE_RESPONSE" | jq -r '.errors[]?.message // "Unknown error"'
              ((FAILED_COUNT++))
            fi
            
            sleep 1  # Rate limiting
          done
          
          echo ""
          echo "📊 DNS Update Summary for ${{ env.SERVICE_NAME }}:"
          echo "   ✅ Successfully updated: $UPDATED_COUNT domains"
          echo "   ❌ Failed to update: $FAILED_COUNT domains"
          echo "   🔗 Target IP: $UPDATE_IP"
          echo "   ⏱️ TTL: 120 seconds (fast propagation)"
          
          if [[ $FAILED_COUNT -gt 0 ]]; then
            echo "⚠️ Some DNS updates failed - check logs above"
          fi
            echo "   • Direct: http://$TAILSCALE_IP"
            echo "   • Domain: http://$DOMAIN_NAME (after DNS propagation)"
            echo ""
            echo "⚠️ Note: This domain only resolves within the Tailscale network for security"
          else
            echo "❌ DNS record update failed:"
            echo "$UPDATE_RESPONSE" | jq -r '.errors[]?.message // "Unknown error"'
            exit 1
          fi

  # ============================================================================
  # Health Checks
  # ============================================================================
  health-check:
    name: 🏥 Health Check
    runs-on: ubuntu-latest
    needs: [preflight-checks, setup-infrastructure, deploy-service]
    if: always() && (needs.deploy-service.result == 'success' || needs.preflight-checks.outputs.should_health_check == 'true')
    
    steps:
      - name: 🏥 Perform Health Checks
        run: |
          echo "🏥 Running health checks..."
          
          SERVER_IP="${{ needs.setup-infrastructure.outputs.server_ip }}"
          
          if [[ -n "$SERVER_IP" ]]; then
            echo "Testing SSH connectivity..."
            ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no -o ConnectTimeout=10 root@$SERVER_IP "echo 'SSH OK'"
            
            echo "Checking services..."
            ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@$SERVER_IP "systemctl is-active docker"
            ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@$SERVER_IP "tailscale status --peers=false"
            ssh -i ~/.ssh/linode_deployment_key -o StrictHostKeyChecking=no root@$SERVER_IP "docker ps"
            
            echo "✅ Health checks passed"
          else
            echo "⚠️ No server IP available for health checks"
          fi

  # ============================================================================
  # Notifications
  # ============================================================================
  notify:
    name: 📢 Notify
    runs-on: ubuntu-latest
    needs: [preflight-checks, deploy-service, destroy-service, health-check]
    if: always()
    
    steps:
      - name: 📢 Send Notification
        env:
          DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK }}
        if: env.DISCORD_WEBHOOK != ''
        run: |
          echo "📢 Sending notification..."
          
          # Determine overall status
          if [[ "${{ needs.deploy-service.result }}" == "success" || "${{ needs.destroy-service.result }}" == "success" ]]; then
            STATUS="✅ SUCCESS"
            COLOR="3066993"
          else
            STATUS="❌ FAILED"
            COLOR="15158332"
          fi
          
          ACTION_EMOJI=""
          case "${{ env.ACTION_TYPE }}" in
            "deploy") ACTION_EMOJI="🚀" ;;
            "destroy") ACTION_EMOJI="🗑️" ;;
            "health-check") ACTION_EMOJI="🏥" ;;
            "restart") ACTION_EMOJI="🔄" ;;
          esac
          
          curl -H "Content-Type: application/json" \
            -d "{
              \"embeds\": [{
                \"title\": \"$ACTION_EMOJI $STATUS: ${{ env.SERVICE_NAME }} ${{ env.ACTION_TYPE }}\",
                \"description\": \"Service: ${{ env.SERVICE_NAME }}\\nAction: ${{ env.ACTION_TYPE }}\\nMode: ${{ env.DEPLOYMENT_MODE }}\\nTests Skipped: ${{ env.SKIP_TESTS }}\\nDocker Build Skipped: ${{ env.SKIP_DOCKER_BUILD }}\\nServer Overwritten: ${{ env.OVERWRITE_SERVER }}\",
                \"color\": $COLOR,
                \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%S.000Z)\"
              }]
            }" \
            "$DISCORD_WEBHOOK"
